{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmiamen/N8NRepo/blob/main/model_to_build_off.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# üöú Master Agricultural Keywording Pipeline - BULLETPROOF DEBUG Edition\n",
        "\n",
        "**Production-Grade Agricultural Image Keywording System**\n",
        "\n",
        "This notebook orchestrates a bulletproof two-stage agricultural image keywording pipeline with comprehensive DEBUG monitoring. It combines a fine-tuned LLaVA model for specialist keyword generation with GPT-4 Vision refinement, featuring real-time progress tracking and robust error handling.\n",
        "\n",
        "## üéØ **Key Features:**\n",
        "- **Comprehensive DEBUG Logging**: Live progress indicators for AI processing steps\n",
        "- **Memory Management**: GPU monitoring and automatic cleanup\n",
        "- **Business API Integration**: Working Dropbox Business API with fallbacks\n",
        "- **Error Resilience**: Bulletproof JSON formatting and error recovery\n",
        "- **Production Ready**: Shopify-compatible CSV export with photographer matching\n",
        "\n",
        "## üèóÔ∏è **12-Step Pipeline Architecture:**\n",
        "1. Install Dependencies\n",
        "2. Imports, Drive Mount, Config Setup\n",
        "3. Utility Functions + PhotographerMatcher\n",
        "4. Dropbox Business API Integration\n",
        "5. Load Finetuned LLaVA Model\n",
        "6. **üî• Enhanced Specialist Inference** (with comprehensive DEBUG)\n",
        "7. Save Intermediate JSON\n",
        "8. Load + Enrich Intermediate JSON\n",
        "9. **üî• Enhanced GPT-4 Vision Refinement** (with comprehensive DEBUG)\n",
        "10. Assemble Final Metadata Rows\n",
        "11. Final CSV Export (Shopify-compatible)\n",
        "12. Pipeline Summary (complete statistics)\n",
        "\n",
        "---\n",
        "\n",
        "**DEBUG ENHANCEMENTS:**\n",
        "- Real-time progress indicators prevent \"stagnant logs\"\n",
        "- GPU memory monitoring and automatic cleanup\n",
        "- Multi-phase timing for API calls and processing\n",
        "- Success/failure statistics with fallback reporting\n",
        "- ETA calculations for long-running processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_header"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "Installing all required packages with Business API support and memory management tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step1_install"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ AGStock Keyworder Enhanced v5.0 - Installing Dependencies\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "        print(f\"‚úÖ {package}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå Failed: {package}\")\n",
        "        return False\n",
        "\n",
        "dependencies = [\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"dropbox>=11.36.0\",\n",
        "    \"pillow>=10.0.0\",\n",
        "    \"requests>=2.31.0\",\n",
        "    \"python-dotenv>=1.0.0\",\n",
        "    \"openai>=1.0.0\",\n",
        "    \"pandas>=2.0.0\",\n",
        "    \"numpy>=1.24.0\",\n",
        "    \"matplotlib>=3.7.0\",\n",
        "    \"tqdm>=4.65.0\",\n",
        "    \"psutil>=5.9.0\"\n",
        "]\n",
        "\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "failed = []\n",
        "for pkg in dependencies:\n",
        "    if not install_package(pkg):\n",
        "        failed.append(pkg)\n",
        "\n",
        "if failed:\n",
        "    print(f\"‚ö†Ô∏è Failed: {len(failed)} packages\")\n",
        "else:\n",
        "    print(\"‚úÖ All dependencies installed!\")\n",
        "\n",
        "print(\"üéØ v5.0 Optimizations Complete\")\n",
        "print(\"üèÅ Step 1 Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_header"
      },
      "source": [
        "## Step 2: Imports, Drive Mount, and Configuration\n",
        "Setting up all imports with memory management and mounting Google Drive for model access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step2_imports"
      },
      "outputs": [],
      "source": [
        "print(\"üîß AGStock Keyworder Enhanced v4.0 FIXED - Environment Setup\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Core Python libraries\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import base64\n",
        "import logging\n",
        "import requests\n",
        "import dropbox\n",
        "import shutil\n",
        "import gc\n",
        "import unicodedata\n",
        "import random\n",
        "from typing import Dict, List, Any, Optional\n",
        "from functools import wraps\n",
        "\n",
        "# Third-party libraries - REMOVED tenacity dependency for simplified retry system\n",
        "import torch\n",
        "from transformers import LlavaForConditionalGeneration, LlavaProcessor, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "\n",
        "# Enhanced logging setup with comprehensive monitoring\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# SIMPLIFIED RETRY DECORATOR - replaces complex tenacity usage\n",
        "def simple_retry(max_attempts=3, delay=2, backoff=2, max_delay=30):\n",
        "    \"\"\"Simple retry decorator with exponential backoff\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            last_exception = None\n",
        "            current_delay = delay\n",
        "\n",
        "            for attempt in range(max_attempts):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    last_exception = e\n",
        "\n",
        "                    if attempt == max_attempts - 1:  # Last attempt\n",
        "                        raise last_exception\n",
        "\n",
        "                    # Exponential backoff with jitter\n",
        "                    sleep_time = min(current_delay + random.uniform(0, 1), max_delay)\n",
        "                    logging.info(f\"Retry attempt {attempt + 1}/{max_attempts} after {sleep_time:.1f}s delay\")\n",
        "                    time.sleep(sleep_time)\n",
        "                    current_delay *= backoff\n",
        "\n",
        "            raise last_exception  # Should never reach here\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Google Drive mount with error handling\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "    drive.mount('/content/drive')\n",
        "    logging.info(\"‚úÖ Google Drive mounted successfully\")\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    logging.info(\"‚ÑπÔ∏è Running outside Google Colab - local mode\")\n",
        "    IN_COLAB = False\n",
        "    # Create mock files module for local testing\n",
        "    class MockFiles:\n",
        "        @staticmethod\n",
        "        def download(filename):\n",
        "            print(f\"üìÅ Local mode: {filename} ready for manual download\")\n",
        "    files = MockFiles()\n",
        "\n",
        "# Enhanced path configuration with validation\n",
        "if IN_COLAB:\n",
        "    DRIVE_PROJECT_PATH = \"/content/drive/MyDrive/AgriKeywordingProject\"\n",
        "    MODEL_PATH = os.path.join(DRIVE_PROJECT_PATH, \"model_weights\")\n",
        "    PHOTOGRAPHER_CSV_PATH = os.path.join(DRIVE_PROJECT_PATH, \"photographer_matching.csv\")\n",
        "else:\n",
        "    DRIVE_PROJECT_PATH = \"./AgriKeywordingProject\"\n",
        "    MODEL_PATH = \"./model_weights\"\n",
        "    PHOTOGRAPHER_CSV_PATH = \"./photographer_matching.csv\"\n",
        "\n",
        "logging.info(f\"üìÅ Project path: {DRIVE_PROJECT_PATH}\")\n",
        "logging.info(f\"ü§ñ Model path: {MODEL_PATH}\")\n",
        "logging.info(f\"üë• Photographer CSV: {PHOTOGRAPHER_CSV_PATH}\")\n",
        "\n",
        "# Memory management helper functions\n",
        "def get_gpu_memory_usage():\n",
        "    \"\"\"Get current GPU memory usage in GB\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**3\n",
        "    return 0.0\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Clean up GPU and system memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"üíæ Initial GPU Memory: {get_gpu_memory_usage():.2f}GB\")\n",
        "print(\"‚úÖ Enhanced environment initialization complete with SIMPLIFIED retry system\")\n",
        "print(\"üöÄ Ready for utility functions and enhanced features setup\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## Configuration Form\n",
        "Enter your API credentials and specify the Dropbox folder to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_form"
      },
      "outputs": [],
      "source": [
        "#@title Dropbox Business API Configuration\n",
        "DROPBOX_APP_KEY = \"\" #@param {type:\"string\"}\n",
        "DROPBOX_APP_SECRET = \"\" #@param {type:\"string\"}\n",
        "DROPBOX_REFRESH_TOKEN = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@title OpenAI API Configuration\n",
        "OPENAI_API_KEY = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@title Image Source Configuration\n",
        "image_folder_name = \"Images_To_Keyword\" #@param {type:\"string\"}\n",
        "\n",
        "# Set environment variables automatically\n",
        "os.environ['DROPBOX_APP_KEY'] = DROPBOX_APP_KEY\n",
        "os.environ['DROPBOX_APP_SECRET'] = DROPBOX_APP_SECRET\n",
        "os.environ['DROPBOX_REFRESH_TOKEN'] = DROPBOX_REFRESH_TOKEN\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "# Validate configuration\n",
        "config_status = {\n",
        "    \"Dropbox\": bool(DROPBOX_APP_KEY and DROPBOX_APP_SECRET and DROPBOX_REFRESH_TOKEN),\n",
        "    \"OpenAI\": bool(OPENAI_API_KEY),\n",
        "    \"Folder\": bool(image_folder_name)\n",
        "}\n",
        "\n",
        "print(\"üìä Configuration Status:\")\n",
        "for service, status in config_status.items():\n",
        "    status_icon = \"‚úÖ\" if status else \"‚ö†Ô∏è\"\n",
        "    print(f\"   {status_icon} {service}: {'Configured' if status else 'Not configured'}\")\n",
        "\n",
        "print(f\"‚úÖ Dropbox folder: '{image_folder_name}'\")\n",
        "print(f\"‚úÖ API keys configured: {bool(OPENAI_API_KEY)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_header"
      },
      "source": [
        "## Step 3: Utility Functions and PhotographerMatcher Class\n",
        "Core functions for data processing and photographer identification with bulletproof error handling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîß AGStock Keyworder Enhanced v4.0 FIXED - Utility Functions Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# EFFICIENT PhotographerMatcher class - MEMORY BOMB FIXED!\n",
        "class PhotographerMatcher:\n",
        "    \"\"\"Efficient photographer matching with range storage - NO MEMORY EXPANSION\"\"\"\n",
        "\n",
        "    def __init__(self, csv_path):\n",
        "        \"\"\"Initialize efficient photographer matcher\"\"\"\n",
        "        self.ranges = []  # [(start, end, photographer_info)]\n",
        "        self.exact_matches = {}  # {sku: photographer_info}\n",
        "        self.prefixes = {}  # {prefix: photographer_info}\n",
        "        self.load_photographers(csv_path)\n",
        "\n",
        "    def load_photographers(self, csv_path):\n",
        "        \"\"\"Load photographer data with efficient range storage - NO EXPANSION\"\"\"\n",
        "        if not os.path.exists(csv_path):\n",
        "            logging.warning(f\"Photographer CSV not found: {csv_path}\")\n",
        "            print(f\"‚ö†Ô∏è Photographer CSV not found: {csv_path}\")\n",
        "            print(\"üîÑ Continuing with default photographer matching\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            for _, row in df.iterrows():\n",
        "                sku_range = str(row.get('sku_range', ''))\n",
        "                name = str(row.get('name', 'Unknown'))\n",
        "                state = str(row.get('state', 'Unknown'))\n",
        "\n",
        "                photographer_info = {\n",
        "                    'name': name,\n",
        "                    'state': state\n",
        "                }\n",
        "\n",
        "                # Efficient range parsing (e.g., \"270000-279999\")\n",
        "                if '-' in sku_range:\n",
        "                    try:\n",
        "                        start_str, end_str = sku_range.split('-', 1)\n",
        "                        start_num = int(start_str.strip())\n",
        "                        end_num = int(end_str.strip())\n",
        "\n",
        "                        # Store as range tuple - NO EXPANSION\n",
        "                        self.ranges.append((start_num, end_num, photographer_info))\n",
        "\n",
        "                    except ValueError:\n",
        "                        logging.warning(f\"Invalid range format: {sku_range}\")\n",
        "\n",
        "                # Prefix matching (e.g., \"280\", \"AH\")\n",
        "                elif sku_range:\n",
        "                    self.prefixes[sku_range] = photographer_info\n",
        "\n",
        "            print(f\"‚úÖ Loaded {len(df)} photographer entries efficiently\")\n",
        "            print(f\"üìä Ranges: {len(self.ranges)}, Prefixes: {len(self.prefixes)}\")\n",
        "            print(f\"üöÄ MEMORY OPTIMIZED: No dictionary expansion used!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load photographer CSV: {e}\")\n",
        "            print(f\"‚ùå Failed to load photographer CSV: {e}\")\n",
        "\n",
        "    def get_vendor_info(self, sku):\n",
        "        \"\"\"Efficient vendor lookup with range checking - NO MEMORY EXPANSION\"\"\"\n",
        "        if not sku:\n",
        "            return {'vendor': 'Unknown', 'photographer_match': 'no', 'state': 'Unknown'}\n",
        "\n",
        "        sku_str = str(sku).strip()\n",
        "\n",
        "        # Check exact matches first (fastest)\n",
        "        if sku_str in self.exact_matches:\n",
        "            photographer = self.exact_matches[sku_str]\n",
        "            return {\n",
        "                'vendor': photographer['name'],\n",
        "                'photographer_match': 'yes',\n",
        "                'state': photographer['state']\n",
        "            }\n",
        "\n",
        "        # Try to convert to integer for range checking\n",
        "        try:\n",
        "            sku_int = int(sku_str)\n",
        "\n",
        "            # Check ranges efficiently - O(n) but n is small (number of photographers)\n",
        "            for start, end, photographer_info in self.ranges:\n",
        "                if start <= sku_int <= end:\n",
        "                    return {\n",
        "                        'vendor': photographer_info['name'],\n",
        "                        'photographer_match': 'yes',\n",
        "                        'state': photographer_info['state']\n",
        "                    }\n",
        "        except ValueError:\n",
        "            # SKU is not numeric, skip range checking\n",
        "            pass\n",
        "\n",
        "        # Check prefix matches\n",
        "        for prefix, photographer_info in self.prefixes.items():\n",
        "            if sku_str.startswith(prefix):\n",
        "                return {\n",
        "                    'vendor': photographer_info['name'],\n",
        "                    'photographer_match': 'yes',\n",
        "                    'state': photographer_info['state']\n",
        "                }\n",
        "\n",
        "        # No match found\n",
        "        return {'vendor': 'Unknown', 'photographer_match': 'no', 'state': 'Unknown'}\n",
        "\n",
        "\n",
        "#\n",
        "\n",
        "# Enhanced Pipeline Timer for comprehensive performance tracking\n",
        "class EnhancedPipelineTimer:\n",
        "    \"\"\"Enhanced pipeline timing with step-by-step performance analysis\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = {}\n",
        "        self.pipeline_start = time.time()\n",
        "        print(\"‚è±Ô∏è Enhanced Pipeline Timer initialized\")\n",
        "\n",
        "    def start_step(self, step_name):\n",
        "        \"\"\"Start timing a pipeline step\"\"\"\n",
        "        self.steps[step_name] = {'start': time.time(), 'end': None}\n",
        "        logging.info(f\"‚è±Ô∏è Starting step: {step_name}\")\n",
        "\n",
        "    def end_step(self, step_name):\n",
        "        \"\"\"End timing a pipeline step\"\"\"\n",
        "        if step_name in self.steps and self.steps[step_name]['end'] is None:\n",
        "            self.steps[step_name]['end'] = time.time()\n",
        "            duration = self.get_step_duration(step_name)\n",
        "            logging.info(f\"‚è±Ô∏è Completed step: {step_name} in {duration:.1f}s\")\n",
        "        else:\n",
        "            logging.warning(f\"‚ö†Ô∏è Step '{step_name}' not found or already ended\")\n",
        "\n",
        "    def get_step_duration(self, step_name):\n",
        "        \"\"\"Get duration of a completed step\"\"\"\n",
        "        if step_name not in self.steps:\n",
        "            return 0.0\n",
        "        step = self.steps[step_name]\n",
        "        if step['end'] is None:\n",
        "            return time.time() - step['start']  # Currently running\n",
        "        return step['end'] - step['start']\n",
        "\n",
        "    def get_total_time(self):\n",
        "        \"\"\"Get total pipeline execution time\"\"\"\n",
        "        return time.time() - self.pipeline_start\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get comprehensive timing summary\"\"\"\n",
        "        summary = {}\n",
        "        for step_name, step_data in self.steps.items():\n",
        "            summary[step_name] = self.get_step_duration(step_name)\n",
        "        summary['total_time'] = self.get_total_time()\n",
        "        return summary\n",
        "\n",
        "# Initialize enhanced pipeline timer\n",
        "pipeline_timer = EnhancedPipelineTimer()\n",
        "\n",
        "# Enhanced agricultural keyword filtering system\n",
        "def filter_llava_keywords(raw_keywords, max_keywords=12):\n",
        "    \"\"\"Enhanced keyword filtering for LLaVA outputs with agricultural focus\"\"\"\n",
        "    if not raw_keywords or raw_keywords == 'processing_failed':\n",
        "        return 'agriculture, farming'\n",
        "\n",
        "    # Clean and split keywords\n",
        "    cleaned = clean_ai_keywords(raw_keywords)\n",
        "    if len(cleaned) <= max_keywords:\n",
        "        return ', '.join(cleaned)\n",
        "\n",
        "    # Priority scoring for agricultural relevance\n",
        "    priority_keywords = {\n",
        "        # High priority - Core agricultural terms\n",
        "        'agriculture': 10, 'farming': 10, 'cattle': 9, 'corn': 9, 'soybean': 9,\n",
        "        'wheat': 9, 'dairy': 9, 'livestock': 9, 'pasture': 8, 'field': 8,\n",
        "        'crop': 8, 'farm': 8, 'rural': 8, 'barn': 8, 'harvest': 8,\n",
        "\n",
        "        # Medium priority - Specific agricultural elements\n",
        "        'angus': 7, 'holstein': 7, 'hereford': 7, 'charolais': 7, 'simmental': 7,\n",
        "        'jersey': 7, 'brahman': 7, 'shorthorn': 7, 'duroc': 7, 'hampshire': 7,\n",
        "        'yorkshire': 7, 'landrace': 7, 'alfalfa': 6, 'hay': 6, 'oats': 6,\n",
        "        'barley': 6, 'rye': 6, 'sorghum': 6, 'canola': 6, 'sunflower': 6,\n",
        "\n",
        "        # Geographic terms - medium priority\n",
        "        'iowa': 6, 'nebraska': 6, 'kansas': 6, 'wisconsin': 6, 'minnesota': 6,\n",
        "        'illinois': 6, 'missouri': 6, 'south dakota': 6, 'north dakota': 6,\n",
        "\n",
        "        # Lower priority - Generic terms\n",
        "        'animal': 4, 'outdoor': 3, 'green': 3, 'brown': 3, 'natural': 3,\n",
        "\n",
        "        # Remove these terms completely\n",
        "        'processing': 0, 'computer': 0, 'technical': 0, 'system': 0, 'operation': 0,\n",
        "        'focus': 0, 'image': 0, 'photo': 0, 'picture': 0, 'digital': 0\n",
        "    }\n",
        "\n",
        "    # Score and sort keywords\n",
        "    scored_keywords = []\n",
        "    for keyword in cleaned:\n",
        "        # Remove redundant terms first\n",
        "        if any(duplicate in keyword for duplicate in ['cattle cattle', 'farm farm', 'agriculture agriculture']):\n",
        "            continue\n",
        "\n",
        "        score = priority_keywords.get(keyword.lower(), 5)  # Default medium score\n",
        "        if score > 0:  # Only include non-blacklisted terms\n",
        "            scored_keywords.append((keyword, score))\n",
        "\n",
        "    # Sort by score (highest first) and take top max_keywords\n",
        "    scored_keywords.sort(key=lambda x: x[1], reverse=True)\n",
        "    final_keywords = [kw for kw, score in scored_keywords[:max_keywords]]\n",
        "\n",
        "    # Ensure minimum keyword count with fallbacks\n",
        "    if len(final_keywords) < 8:\n",
        "        fallback_keywords = ['agriculture', 'farming', 'crop', 'field', 'rural', 'farm']\n",
        "        for fallback in fallback_keywords:\n",
        "            if fallback not in final_keywords and len(final_keywords) < 8:\n",
        "                final_keywords.append(fallback)\n",
        "\n",
        "    return ', '.join(final_keywords)\n",
        "\n",
        "def correct_geographic_keywords(keywords, photographer_context):\n",
        "    \"\"\"Enhanced geographic correction with photographer context awareness\"\"\"\n",
        "    if not keywords:\n",
        "        return keywords\n",
        "\n",
        "    photographer_state = photographer_context.get('state', '').lower()\n",
        "    if not photographer_state or photographer_state == 'unknown':\n",
        "        return keywords  # No correction possible\n",
        "\n",
        "    keyword_list = [kw.strip().lower() for kw in keywords.split(',')]\n",
        "\n",
        "    # US state mappings for correction\n",
        "    state_corrections = {\n",
        "        'iowa': ['illinois', 'nebraska', 'minnesota', 'wisconsin'],\n",
        "        'wisconsin': ['iowa', 'illinois', 'minnesota', 'michigan'],\n",
        "        'nebraska': ['iowa', 'kansas', 'colorado', 'south dakota'],\n",
        "        'kansas': ['nebraska', 'oklahoma', 'missouri', 'colorado'],\n",
        "        'illinois': ['iowa', 'wisconsin', 'indiana', 'missouri'],\n",
        "        'minnesota': ['wisconsin', 'iowa', 'north dakota', 'south dakota'],\n",
        "        'missouri': ['kansas', 'illinois', 'arkansas', 'iowa'],\n",
        "        'texas': ['oklahoma', 'louisiana', 'arkansas', 'new mexico'],\n",
        "        'california': ['nevada', 'oregon', 'arizona'],\n",
        "        'florida': ['georgia', 'alabama'],\n",
        "        'ohio': ['michigan', 'indiana', 'pennsylvania', 'west virginia']\n",
        "    }\n",
        "\n",
        "    # Apply geographic correction\n",
        "    corrected_keywords = []\n",
        "    photographer_state_added = False\n",
        "\n",
        "    for keyword in keyword_list:\n",
        "        # Check if keyword is an incorrect state\n",
        "        incorrect_state = False\n",
        "        for correct_state, incorrect_states in state_corrections.items():\n",
        "            if correct_state == photographer_state and keyword in incorrect_states:\n",
        "                # Replace incorrect state with correct one\n",
        "                if not photographer_state_added:\n",
        "                    corrected_keywords.append(photographer_state)\n",
        "                    photographer_state_added = True\n",
        "                incorrect_state = True\n",
        "                break\n",
        "\n",
        "        if not incorrect_state:\n",
        "            corrected_keywords.append(keyword)\n",
        "\n",
        "    # Ensure photographer state is first if known\n",
        "    if photographer_state and photographer_state != 'unknown':\n",
        "        if photographer_state in corrected_keywords:\n",
        "            corrected_keywords.remove(photographer_state)\n",
        "        corrected_keywords.insert(0, photographer_state)\n",
        "\n",
        "    return ', '.join(corrected_keywords)\n",
        "\n",
        "def remove_redundant_terms(keywords):\n",
        "    \"\"\"Enhanced redundancy removal with agricultural term intelligence\"\"\"\n",
        "    if not keywords:\n",
        "        return []\n",
        "\n",
        "    # Convert to list if string\n",
        "    if isinstance(keywords, str):\n",
        "        keyword_list = [kw.strip() for kw in keywords.split(',')]\n",
        "    else:\n",
        "        keyword_list = keywords\n",
        "\n",
        "    # Remove obvious duplicates and redundant variations\n",
        "    redundancy_groups = {\n",
        "        'cattle': ['cattle cattle', 'cow cattle', 'cattle cow', 'livestock cattle'],\n",
        "        'agriculture': ['agriculture agriculture', 'agricultural agriculture', 'farming agriculture'],\n",
        "        'farm': ['farm farm', 'farming farm', 'farm farming'],\n",
        "        'crop': ['crop crop', 'crops crop', 'crop crops'],\n",
        "        'field': ['field field', 'fields field', 'field fields']\n",
        "    }\n",
        "\n",
        "    # Technical terms to remove\n",
        "    technical_blacklist = [\n",
        "        'processing', 'computer', 'technical', 'operation', 'focus', 'system',\n",
        "        'digital', 'image', 'photo', 'picture', 'analysis', 'data'\n",
        "    ]\n",
        "\n",
        "    filtered_keywords = []\n",
        "    for keyword in keyword_list:\n",
        "        keyword_lower = keyword.lower().strip()\n",
        "\n",
        "        # Skip empty keywords\n",
        "        if not keyword_lower:\n",
        "            continue\n",
        "\n",
        "        # Skip technical blacklist terms\n",
        "        if keyword_lower in technical_blacklist:\n",
        "            continue\n",
        "\n",
        "        # Check for redundancy patterns\n",
        "        is_redundant = False\n",
        "        for base_term, redundant_variations in redundancy_groups.items():\n",
        "            if keyword_lower in redundant_variations:\n",
        "                # Add base term if not already present\n",
        "                if base_term not in [k.lower() for k in filtered_keywords]:\n",
        "                    filtered_keywords.append(base_term)\n",
        "                is_redundant = True\n",
        "                break\n",
        "\n",
        "        # Add keyword if not redundant and not already present\n",
        "        if not is_redundant and keyword_lower not in [k.lower() for k in filtered_keywords]:\n",
        "            filtered_keywords.append(keyword)\n",
        "\n",
        "    return filtered_keywords\n",
        "\n",
        "# Enhanced CSV download function with bulletproof error handling\n",
        "def bulletproof_download_csv(filename, description=\"CSV file\"):\n",
        "    \"\"\"Bulletproof CSV download with multiple fallback strategies\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚ùå File not found: {filename}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Primary: Google Colab files.download()\n",
        "        if IN_COLAB:\n",
        "            files.download(filename)\n",
        "            print(f\"‚úÖ {description} downloaded via Colab\")\n",
        "            return True\n",
        "        else:\n",
        "            # Local mode: Just confirm file is ready\n",
        "            file_size = os.path.getsize(filename)\n",
        "            print(f\"‚úÖ {description} ready for download: {filename} ({file_size:,} bytes)\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Download method failed: {e}\")\n",
        "        print(f\"üìÅ File available locally: {filename}\")\n",
        "\n",
        "        # Fallback: Print file location\n",
        "        abs_path = os.path.abspath(filename)\n",
        "        print(f\"üìÇ Absolute path: {abs_path}\")\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Enhanced utility functions initialized\")\n",
        "print(\"üë• PhotographerMatcher class with simplified exact matching ready\")\n",
        "print(\"‚è±Ô∏è Enhanced pipeline timer activated\")\n",
        "print(\"üîß Agricultural keyword filtering system loaded\")\n",
        "print(\"üåç Geographic correction algorithms ready\")\n",
        "print(\"üõ°Ô∏è Bulletproof CSV download system ready\")\n",
        "print(\"üßπ Enhanced redundancy removal ready\")"
      ],
      "metadata": {
        "id": "wAJR1EttgcCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step3_utilities"
      },
      "outputs": [],
      "source": [
        "# Bulletproof string escaping for JSON safety\n",
        "def bulletproof_escape_string(text):\n",
        "    \"\"\"Bulletproof string escaping to prevent JSON corruption\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Critical JSON escaping\n",
        "    escape_map = {\n",
        "        '\\\\': '\\\\\\\\',\n",
        "        '\"': '\\\\\"',\n",
        "        '\\n': '\\\\n',\n",
        "        '\\r': '\\\\r',\n",
        "        '\\t': '\\\\t',\n",
        "        '\\b': '\\\\b',\n",
        "        '\\f': '\\\\f'\n",
        "    }\n",
        "\n",
        "    for char, escape in escape_map.items():\n",
        "        text = text.replace(char, escape)\n",
        "    return text\n",
        "\n",
        "# Enhanced agricultural terminology validation system\n",
        "def count_agricultural_terms(text):\n",
        "    \"\"\"Count and analyze agricultural technical terms in text for quality validation\"\"\"\n",
        "    if not text:\n",
        "        return {\"count\": 0, \"terms\": [], \"categories\": {}}\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Comprehensive agricultural terminology database\n",
        "    agricultural_terms = {\n",
        "        'livestock_breeds': [\n",
        "            'hereford', 'holstein', 'simmental', 'angus', 'charolais', 'jersey',\n",
        "            'duroc', 'hampshire', 'yorkshire', 'landrace', 'brahman', 'shorthorn'\n",
        "        ],\n",
        "        'equipment': [\n",
        "            'combine', 'auger wagon', 'sprayer boom', 'planter', 'feed bunk',\n",
        "            'tmr mixer', 'grain cart', 'cultivator', 'disc harrow', 'seed drill',\n",
        "            'manure spreader', 'hay baler', 'mower', 'tedder', 'rake'\n",
        "        ],\n",
        "        'activities': [\n",
        "            'cow-calf operation', 'calving', 'nursing', 'vaccination', 'sidedressing',\n",
        "            'cultivation', 'harvest', 'planting', 'spraying', 'feeding', 'milking',\n",
        "            'breeding', 'weaning', 'castration', 'dehorning'\n",
        "        ],\n",
        "        'crops': [\n",
        "            'corn', 'maize', 'soybean', 'wheat', 'alfalfa', 'hay', 'oats', 'barley',\n",
        "            'rye', 'sorghum', 'millet', 'canola', 'sunflower', 'cotton'\n",
        "        ],\n",
        "        'growth_stages': [\n",
        "            'v1', 'v2', 'v4', 'v6', 'v8', 'v10', 'v12', 'vt', 'r1', 'r2', 'r3',\n",
        "            'r4', 'r5', 'r6', 'boot stage', 'heading', 'tasseling', 'silking',\n",
        "            'milk stage', 'dough stage', 'maturity'\n",
        "        ],\n",
        "        'general_agriculture': [\n",
        "            'pasture', 'livestock', 'cattle', 'farming', 'agriculture', 'crop',\n",
        "            'field', 'barn', 'silo', 'feedlot', 'dairy', 'ranch', 'farm'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    found_terms = []\n",
        "    category_counts = {}\n",
        "\n",
        "    for category, terms in agricultural_terms.items():\n",
        "        category_terms = []\n",
        "        for term in terms:\n",
        "            if term in text_lower:\n",
        "                found_terms.append(term)\n",
        "                category_terms.append(term)\n",
        "        category_counts[category] = len(category_terms)\n",
        "\n",
        "    return {\n",
        "        \"count\": len(found_terms),\n",
        "        \"terms\": found_terms,\n",
        "        \"categories\": category_counts,\n",
        "        \"quality_score\": min(len(found_terms) / 5.0, 1.0)  # Quality score 0-1\n",
        "    }\n",
        "\n",
        "# Enhanced keyword cleaning with agricultural preservation\n",
        "def clean_ai_keywords(keywords_input):\n",
        "    \"\"\"Enhanced keyword cleaning with agricultural term preservation\"\"\"\n",
        "    if not keywords_input:\n",
        "        return []\n",
        "\n",
        "    # Handle different input types\n",
        "    if isinstance(keywords_input, list):\n",
        "        keywords_string = \", \".join(str(kw) for kw in keywords_input)\n",
        "    else:\n",
        "        keywords_string = str(keywords_input)\n",
        "\n",
        "    # Remove conversational filler patterns\n",
        "    junk_patterns = [\n",
        "        r\"an analysis of this agricultural.*?\",\n",
        "        r\"another \\d+.*?\",\n",
        "        r\"<<\\|end\\|>>\",\n",
        "        r\"ASSISTANT:\",\n",
        "        r\"USER:\",\n",
        "        r\"<image>\",\n",
        "        r\"\\[.*?\\]\",\n",
        "        r\"\\{.*?\\}\"\n",
        "    ]\n",
        "\n",
        "    for pattern in junk_patterns:\n",
        "        keywords_string = re.sub(pattern, \"\", keywords_string, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove non-ASCII characters\n",
        "    keywords_string = re.sub('[^\\\\x00-\\\\x7F]+', '', keywords_string)\n",
        "\n",
        "    # Clean and normalize keywords\n",
        "    keywords = [kw.strip().lower() for kw in keywords_string.split(',')]\n",
        "    cleaned_keywords = [kw for kw in keywords if kw and len(kw.strip()) > 1]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    return list(dict.fromkeys(cleaned_keywords))\n",
        "\n",
        "def extract_sku_from_filename(filename):\n",
        "    \"\"\"Extract SKU using enhanced regex patterns with robust error handling\"\"\"\n",
        "    if not filename:\n",
        "        return None\n",
        "\n",
        "    patterns = [\n",
        "        r'[A-Z]{2} (\\d{6})',     # Primary: AH 270480\n",
        "        r'([A-Z]{2})(\\d{6})',    # Secondary: AH270480\n",
        "        r'(\\d{6})',              # Basic: 270480\n",
        "        r'[A-Z]+_(\\d{5,7})',     # Variable length: AB_12345\n",
        "        r'([A-Z]+)(\\d{5,7})',    # No underscore: AB12345\n",
        "        r'\\b(\\d{5,7})\\b',        # Word boundary: any 5-7 digits\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        try:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                groups = match.groups()\n",
        "                if len(groups) == 1:\n",
        "                    return groups[0]\n",
        "                elif len(groups) == 2:\n",
        "                    # Return the numeric part (second group for patterns with prefix)\n",
        "                    return groups[1] if groups[1].isdigit() else groups[0]\n",
        "                elif groups:\n",
        "                    # Return first non-empty group\n",
        "                    return next(g for g in groups if g)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Regex error in extract_sku_from_filename for pattern {pattern}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# AI-Driven State Inference\n",
        "def infer_photographer_state(ai_generated_keywords, photographer_context):\n",
        "    \"\"\"AI-driven state inference\"\"\"\n",
        "\n",
        "    # Get initial photographer state\n",
        "    photographer_state = photographer_context.get('state', 'Unknown')\n",
        "\n",
        "    if photographer_state == 'Unknown' or not photographer_state:\n",
        "        # Use AI keywords to infer state\n",
        "        state_keywords = {\n",
        "            'Iowa': ['corn', 'soybean', 'iowa', 'midwest'],\n",
        "            'Nebraska': ['cattle', 'beef', 'nebraska', 'ranch'],\n",
        "            'Kansas': ['wheat', 'grain', 'kansas', 'prairie'],\n",
        "            'Texas': ['cotton', 'longhorn', 'texas', 'ranch'],\n",
        "            'California': ['vineyard', 'fruit', 'california', 'orchard']\n",
        "        }\n",
        "\n",
        "        keywords_lower = ai_generated_keywords.lower() if ai_generated_keywords else ''\n",
        "\n",
        "        for state, indicators in state_keywords.items():\n",
        "            if any(indicator in keywords_lower for indicator in indicators):\n",
        "                return state\n",
        "\n",
        "        return 'Unknown'\n",
        "\n",
        "    # Use photographer states dictionary for multiple states\n",
        "    if ',' in photographer_state or ' and ' in photographer_state:\n",
        "        states_list = re.split(r',| and |/|&|\\||;', photographer_state)\n",
        "        # Use first state as primary\n",
        "        return states_list[0].strip() if states_list else photographer_state\n",
        "\n",
        "    return photographer_state\n",
        "\n",
        "# Progressive JSON validation with Unicode support\n",
        "def progressive_json_validation(data, context_name=\"Unknown\"):\n",
        "    \"\"\"Validate JSON at multiple stages with Unicode preservation\"\"\"\n",
        "    try:\n",
        "        if not isinstance(data, (list, dict)):\n",
        "            return False, f\"Invalid data type: {type(data)}\"\n",
        "\n",
        "        # JSON serialization test with Unicode preservation\n",
        "        json_content = json.dumps(data, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Round-trip validation\n",
        "        parsed_back = json.loads(json_content)\n",
        "\n",
        "        # Content integrity check\n",
        "        if isinstance(data, list) and len(parsed_back) != len(data):\n",
        "            return False, f\"Content corruption: expected {len(data)} items, got {len(parsed_back)}\"\n",
        "\n",
        "        logging.info(f\"‚úÖ Progressive JSON validation passed for {context_name}\")\n",
        "        return True, \"Validation passed\"\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        return False, f\"JSON encoding error: {e}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Validation error: {e}\"\n",
        "\n",
        "# Critical validation function\n",
        "def validate_data_structure(data, step_name):\n",
        "    \"\"\"Critical validation points at pipeline transitions\"\"\"\n",
        "    if not data:\n",
        "        logging.error(f\"{step_name}: No data to process\")\n",
        "        return False\n",
        "    if not isinstance(data, list):\n",
        "        logging.error(f\"{step_name}: Data should be a list, got {type(data)}\")\n",
        "        return False\n",
        "\n",
        "    logging.info(f\"‚úÖ {step_name}: Validated {len(data)} records\")\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_header"
      },
      "source": [
        "## Step 4: Dropbox Business API Integration\n",
        "Production-ready Dropbox Business API with comprehensive error handling and local fallback support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step4_dropbox"
      },
      "outputs": [],
      "source": [
        "print(\"üîß AGStock Keyworder Enhanced v4.0 FIXED - Dropbox Business API Integration\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "pipeline_timer.start_step(\"Dropbox Integration\")\n",
        "\n",
        "# SIMPLIFIED OAuth2 token refresh - replaces complex tenacity decorator\n",
        "@simple_retry(max_attempts=3, delay=2, backoff=2)\n",
        "def refresh_access_token(app_key, app_secret, refresh_token):\n",
        "    \"\"\"Simplified OAuth2 token refresh with standard retry\"\"\"\n",
        "    if not all([app_key, app_secret, refresh_token]):\n",
        "        logging.error(\"‚ùå Missing Dropbox credentials for token refresh\")\n",
        "        return None\n",
        "\n",
        "    response = requests.post(\n",
        "        'https://api.dropboxapi.com/oauth2/token',\n",
        "        data={\n",
        "            'grant_type': 'refresh_token',\n",
        "            'refresh_token': refresh_token,\n",
        "            'client_id': app_key,\n",
        "            'client_secret': app_secret\n",
        "        },\n",
        "        timeout=30\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    new_access_token = response.json().get('access_token')\n",
        "\n",
        "    if new_access_token:\n",
        "        logging.info(\"‚úÖ Successfully refreshed Dropbox access token\")\n",
        "        return new_access_token\n",
        "    else:\n",
        "        logging.error(\"‚ùå Token refresh response missing access token\")\n",
        "        return None\n",
        "\n",
        "# SIMPLIFIED file download - replaces complex tenacity decorator\n",
        "@simple_retry(max_attempts=3, delay=2, backoff=2)\n",
        "def download_file_simple(url: str, headers: Dict[str, str], local_path: str) -> None:\n",
        "    \"\"\"Simplified file download with standard retry\"\"\"\n",
        "    with requests.post(url, headers=headers, stream=True, timeout=60) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(local_path, 'wb') as f:\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "def download_dropbox_images_enhanced(folder_name: str, local_dir: str = \"_INPUT_IMAGES/\") -> tuple:\n",
        "    \"\"\"Enhanced Dropbox Business API integration with SIMPLIFIED retry logic\"\"\"\n",
        "    download_stats = {\n",
        "        'attempted': 0,\n",
        "        'successful': 0,\n",
        "        'failed': 0,\n",
        "        'skipped': 0,\n",
        "        'start_time': time.time(),\n",
        "        'memory_usage': {\n",
        "            'start': psutil.virtual_memory().percent,\n",
        "            'peak': psutil.virtual_memory().percent\n",
        "        }\n",
        "    }\n",
        "\n",
        "    logging.info(f\"üîÑ Starting Enhanced Business API download for folder: '{folder_name}'\")\n",
        "\n",
        "    # Local test mode for development\n",
        "    if folder_name == \"test_images\" and os.path.exists(\"test_images\"):\n",
        "        logging.info(\"üß™ Entering local test mode\")\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        test_images = [f for f in os.listdir(\"test_images\")\n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))]\n",
        "\n",
        "        if not test_images:\n",
        "            logging.warning(\"‚ö†Ô∏è Local 'test_images' folder is empty\")\n",
        "            return False, download_stats\n",
        "\n",
        "        for img in test_images:\n",
        "            shutil.copy2(os.path.join(\"test_images\", img), os.path.join(local_dir, img))\n",
        "            download_stats['successful'] += 1\n",
        "\n",
        "        download_stats['attempted'] = len(test_images)\n",
        "        logging.info(f\"‚úÖ Copied {len(test_images)} files from test mode\")\n",
        "        return True, download_stats\n",
        "\n",
        "    # Enhanced Business API authentication\n",
        "    if not all([DROPBOX_APP_KEY, DROPBOX_APP_SECRET, DROPBOX_REFRESH_TOKEN]):\n",
        "        logging.error(\"‚ùå Missing Dropbox credentials\")\n",
        "        return False, download_stats\n",
        "\n",
        "    access_token = refresh_access_token(DROPBOX_APP_KEY, DROPBOX_APP_SECRET, DROPBOX_REFRESH_TOKEN)\n",
        "    if not access_token:\n",
        "        logging.error(\"‚ùå Could not obtain valid Dropbox access token\")\n",
        "        return False, download_stats\n",
        "\n",
        "    try:\n",
        "        # Enhanced Business Team Authentication\n",
        "        team_dbx = dropbox.DropboxTeam(access_token)\n",
        "        members = team_dbx.team_members_list()\n",
        "\n",
        "        if not members.members:\n",
        "            logging.error(\"‚ùå No team members found in Dropbox Business account\")\n",
        "            return False, download_stats\n",
        "\n",
        "        member = members.members[0]\n",
        "        member_dbx = team_dbx.as_user(member.profile.team_member_id)\n",
        "        logging.info(f\"ü§ù Authenticated as team member: {member.profile.name.display_name}\")\n",
        "\n",
        "        # Enhanced shared folder discovery with namespace support\n",
        "        shared_folders = member_dbx.sharing_list_folders().entries\n",
        "\n",
        "        # Handle nested folder paths (e.g., \"Batches/25-06-15/Social\")\n",
        "        root_folder_name = folder_name.split('/')[0]\n",
        "        nested_path = '/'.join(folder_name.split('/')[1:]) if '/' in folder_name else ''\n",
        "\n",
        "        target_folder = next((f for f in shared_folders\n",
        "                            if f.name.lower() == root_folder_name.lower()), None)\n",
        "\n",
        "        if not target_folder:\n",
        "            logging.error(f\"‚ùå Root folder '{root_folder_name}' not found in shared folders\")\n",
        "            available_folders = [f.name for f in shared_folders[:5]]\n",
        "            logging.info(f\"üìÅ Available folders: {available_folders}\")\n",
        "            return False, download_stats\n",
        "\n",
        "        logging.info(f\"üìÅ Found target folder '{target_folder.name}' with namespace ID: {target_folder.shared_folder_id}\")\n",
        "\n",
        "        # Enhanced namespace-based file listing\n",
        "        path_to_list = f'/{nested_path}' if nested_path else ''\n",
        "\n",
        "        # Business API headers with namespace context\n",
        "        headers = {\n",
        "            'Authorization': f'Bearer {access_token}',\n",
        "            'Content-Type': 'application/json',\n",
        "            'Dropbox-API-Select-User': member.profile.team_member_id,\n",
        "            'Dropbox-API-Path-Root': json.dumps({\n",
        "                \".tag\": \"namespace_id\",\n",
        "                \"namespace_id\": target_folder.shared_folder_id\n",
        "            })\n",
        "        }\n",
        "\n",
        "        response = requests.post('https://api.dropboxapi.com/2/files/list_folder',\n",
        "                               headers=headers,\n",
        "                               json={'path': path_to_list},\n",
        "                               timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        entries = response.json().get('entries', [])\n",
        "        image_files = [e for e in entries\n",
        "                      if e.get('.tag') == 'file' and\n",
        "                      e.get('name', '').lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp'))]\n",
        "\n",
        "        if not image_files:\n",
        "            logging.warning(f\"‚ö†Ô∏è No image files found in folder: '{folder_name}'\")\n",
        "            return False, download_stats\n",
        "\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        # Enhanced batch download with progress tracking\n",
        "        download_stats['attempted'] = len(image_files)\n",
        "\n",
        "        for i, entry in enumerate(tqdm(image_files, desc=\"Downloading images\"), 1):\n",
        "            file_path = entry.get('path_display')\n",
        "            local_path = os.path.join(local_dir, entry.get('name'))\n",
        "\n",
        "            # Update memory monitoring\n",
        "            current_memory = psutil.virtual_memory().percent\n",
        "            download_stats['memory_usage']['peak'] = max(download_stats['memory_usage']['peak'], current_memory)\n",
        "\n",
        "            if os.path.exists(local_path):\n",
        "                logging.debug(f\"‚è≠Ô∏è Skipping existing file ({i}/{len(image_files)}): {entry.get('name')}\")\n",
        "                download_stats['skipped'] += 1\n",
        "                continue\n",
        "\n",
        "            logging.info(f\"‚¨áÔ∏è Downloading ({i}/{len(image_files)}): '{entry.get('name')}'...\")\n",
        "            download_headers = {\n",
        "                'Authorization': f'Bearer {access_token}',\n",
        "                'Dropbox-API-Select-User': member.profile.team_member_id,\n",
        "                'Dropbox-API-Arg': json.dumps({'path': file_path}),\n",
        "                'Dropbox-API-Path-Root': headers['Dropbox-API-Path-Root']\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                download_file_simple('https://content.dropboxapi.com/2/files/download',\n",
        "                                   download_headers, local_path)\n",
        "                download_stats['successful'] += 1\n",
        "            except Exception as e:\n",
        "                logging.error(f\"‚ùå Failed to download {entry.get('name')}: {e}\")\n",
        "                download_stats['failed'] += 1\n",
        "\n",
        "        total_time = time.time() - download_stats['start_time']\n",
        "        success_rate = (download_stats['successful'] / download_stats['attempted']) * 100 if download_stats['attempted'] > 0 else 0\n",
        "\n",
        "        logging.info(f\"‚úÖ Download complete: {download_stats['successful']}/{download_stats['attempted']} files ({success_rate:.1f}%) in {total_time:.1f}s\")\n",
        "        return download_stats['successful'] > 0, download_stats\n",
        "\n",
        "    except dropbox.exceptions.AuthError as e:\n",
        "        logging.error(f\"‚ùå Dropbox authentication error: {e}\")\n",
        "        return False, download_stats\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Business API error: {e}\")\n",
        "        return False, download_stats\n",
        "\n",
        "# Execute download step\n",
        "print(\"üî• EXECUTING ENHANCED DROPBOX DOWNLOAD\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "download_success, download_stats = download_dropbox_images_enhanced(image_folder_name, \"_INPUT_IMAGES/\")\n",
        "\n",
        "if download_success:\n",
        "    print(f\"‚úÖ Download completed successfully!\")\n",
        "    print(f\"üìä Downloaded: {download_stats['successful']} files\")\n",
        "    print(f\"‚è≠Ô∏è Skipped: {download_stats['skipped']} existing files\")\n",
        "    print(f\"‚ùå Failed: {download_stats['failed']} files\")\n",
        "    print(f\"üíæ Memory usage: {download_stats['memory_usage']['start']:.1f}% ‚Üí {download_stats['memory_usage']['peak']:.1f}%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Download failed, checking for existing images...\")\n",
        "\n",
        "# Check available images\n",
        "try:\n",
        "    if os.path.exists(\"_INPUT_IMAGES/\"):\n",
        "        image_files = [f for f in os.listdir(\"_INPUT_IMAGES/\") if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        print(f\"üìä Images available for processing: {len(image_files)}\")\n",
        "        if image_files:\n",
        "            print(f\"üìÅ Sample files: {', '.join(image_files[:3])}{'...' if len(image_files) > 3 else ''}\")\n",
        "    else:\n",
        "        print(\"üìÅ No _INPUT_IMAGES/ directory found\")\n",
        "        image_files = []\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error checking image files: {e}\")\n",
        "    image_files = []\n",
        "\n",
        "pipeline_timer.end_step(\"Dropbox Integration\")\n",
        "\n",
        "print(\"‚úÖ Enhanced Dropbox Business API integration complete\")\n",
        "print(\"üîß SIMPLIFIED OAuth2 token refresh with standard retry\")\n",
        "print(\"üè¢ Team namespace support and member delegation configured\")\n",
        "print(\"üìÅ Nested folder path support with comprehensive error handling\")\n",
        "print(\"üõ°Ô∏è Local test mode fallback and enhanced metrics tracking\")\n",
        "print(f\"üíæ Memory monitoring with psutil integrated\")\n",
        "print(f\"‚è±Ô∏è Step duration: {pipeline_timer.get_step_duration('Dropbox Integration'):.1f} seconds\")\n",
        "\n",
        "print(\"\" + \"=\" * 65)\n",
        "print(\"üèÅ Step 4 Complete: SIMPLIFIED Dropbox Business API Ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_header"
      },
      "source": [
        "## Step 5: Load Finetuned LLaVA Model\n",
        "Loading the base LLaVA model with PEFT adapter from Google Drive with comprehensive validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step5_model_loading"
      },
      "outputs": [],
      "source": [
        "print(\"üîß AGStock Keyworder Enhanced v4.0 FIXED - Loading Finetuned LLaVA Model\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "pipeline_timer.start_step(\"Model Loading\")\n",
        "\n",
        "def validate_model_paths():\n",
        "    \"\"\"Validate all required paths exist before model loading\"\"\"\n",
        "    print(\"üîç Validating model paths...\")\n",
        "\n",
        "    if not os.path.exists(DRIVE_PROJECT_PATH):\n",
        "        print(f\"‚ùå Project directory not found: {DRIVE_PROJECT_PATH}\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(f\"‚ùå Model weights directory not found: {MODEL_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Check for required PEFT adapter files\n",
        "    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n",
        "    missing_files = []\n",
        "\n",
        "    for file in required_files:\n",
        "        file_path = os.path.join(MODEL_PATH, file)\n",
        "        if not os.path.exists(file_path):\n",
        "            missing_files.append(file)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"‚ùå Missing PEFT adapter files: {missing_files}\")\n",
        "        return False\n",
        "\n",
        "    print(\"‚úÖ All required model files found\")\n",
        "    return True\n",
        "\n",
        "# Validate paths before loading\n",
        "if not validate_model_paths():\n",
        "    print(\"üí° Please ensure the finetuned model weights are properly uploaded to Google Drive\")\n",
        "    print(f\"   Expected location: {MODEL_PATH}\")\n",
        "    raise Exception(\"Model validation failed\")\n",
        "\n",
        "# Enhanced BitsAndBytesConfig with BUILD_SPEC.md requirements\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,  # NEW: Enhanced quantization accuracy\n",
        "    bnb_4bit_quant_type=\"nf4\"        # NEW: NF4 quantization for better performance\n",
        ")\n",
        "\n",
        "print(f\"üîß Loading base LLaVA model: {model_id}\")\n",
        "print(f\"‚öôÔ∏è Using enhanced 4-bit quantization with double quantization and NF4\")\n",
        "print(f\"üíæ Pre-load GPU Memory: {get_gpu_memory_usage():.2f}GB\")\n",
        "\n",
        "base_load_start = time.time()\n",
        "\n",
        "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "base_load_time = time.time() - base_load_start\n",
        "print(f\"‚úÖ Base LLaVA model loaded in {base_load_time:.2f}s\")\n",
        "print(f\"üíæ GPU Memory after base model: {get_gpu_memory_usage():.2f}GB\")\n",
        "\n",
        "print(\"üîß Loading processor...\")\n",
        "processor_start = time.time()\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "processor_load_time = time.time() - processor_start\n",
        "print(f\"‚úÖ Processor loaded in {processor_load_time:.2f}s\")\n",
        "\n",
        "print(f\"üîß Loading PEFT adapter from: {MODEL_PATH}\")\n",
        "adapter_start = time.time()\n",
        "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
        "adapter_load_time = time.time() - adapter_start\n",
        "print(f\"‚úÖ PEFT adapter loaded in {adapter_load_time:.2f}s\")\n",
        "\n",
        "# Enhanced GPU cleanup following BUILD_SPEC.md requirements\n",
        "print(\"üßπ Performing GPU memory cleanup...\")\n",
        "cleanup_start = time.time()\n",
        "torch.cuda.empty_cache()     # Clear GPU cache\n",
        "torch.cuda.synchronize()     # Synchronize CUDA operations\n",
        "cleanup_time = time.time() - cleanup_start\n",
        "print(f\"‚úÖ GPU cleanup completed in {cleanup_time:.3f}s\")\n",
        "\n",
        "# Final validation\n",
        "print(\"üîç Validating finetuned model...\")\n",
        "test_prompt = \"USER: <image>\\nAnalyze this agricultural image and provide relevant keywords.\\nASSISTANT:\"\n",
        "try:\n",
        "    test_inputs = processor(text=test_prompt, images=None, return_tensors=\"pt\")\n",
        "    print(\"‚úÖ Model validation successful\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model validation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "pipeline_timer.end_step(\"Model Loading\")\n",
        "\n",
        "total_load_time = base_load_time + processor_load_time + adapter_load_time\n",
        "final_gpu_memory = get_gpu_memory_usage()\n",
        "\n",
        "print(f\"\\nüéâ Enhanced finetuned specialist model ready!\")\n",
        "print(f\"   üìä Total load time: {total_load_time:.2f}s\")\n",
        "print(f\"   üßπ GPU cleanup time: {cleanup_time:.3f}s\")\n",
        "print(f\"   üíæ Final GPU memory: {final_gpu_memory:.2f}GB\")\n",
        "print(f\"   ‚öôÔ∏è Enhanced quantization: Double-quant + NF4\")\n",
        "print(f\"   üöÄ Ready for agricultural keyword generation!\")\n",
        "print(f\"   ‚è±Ô∏è Step duration: {pipeline_timer.get_step_duration('Model Loading'):.1f} seconds\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 65)\n",
        "print(\"üèÅ Step 5 Complete: Enhanced LLaVA Model Ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6_header"
      },
      "source": [
        "## Step 6: üî• Enhanced LLaVA Specialist Inference with Keyword Filtering\n",
        "Enhanced LLaVA model inference with comprehensive DEBUG monitoring, keyword filtering, and geographic correction system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_keywords_with_finetuned_model_debug(image_path, model, processor):\n",
        "    \"\"\"Enhanced keyword generation with comprehensive DEBUG logging and filtering\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Image loading phase\n",
        "    try:\n",
        "        image_load_start = time.time()\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_load_time = time.time() - image_load_start\n",
        "\n",
        "        # ENHANCED: More constraining prompt for better keyword focus\n",
        "        prompt = \"\"\"USER: <image>\n",
        "Analyze this agricultural image and provide exactly 8-12 essential keywords focusing on:\n",
        "1. Primary subject (cattle, crop, equipment)\n",
        "2. Specific breed/variety if clearly visible\n",
        "3. Basic setting (pasture, barn, field)\n",
        "4. Simple descriptors (calf, feeding, harvesting)\n",
        "\n",
        "Avoid technical jargon, repetitive terms, and processing terminology.\n",
        "Focus on what buyers would search for.\n",
        "\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "        # Input processing phase\n",
        "        processing_start = time.time()\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(\"cuda\")\n",
        "        processing_time = time.time() - processing_start\n",
        "\n",
        "        # Inference phase with memory monitoring\n",
        "        pre_inference_memory = get_gpu_memory_usage()\n",
        "        inference_start = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=80,  # REDUCED: Limit to reduce keyword overflow\n",
        "                do_sample=True,\n",
        "                temperature=0.5,    # REDUCED: More focused responses\n",
        "                top_k=30,          # REDUCED: Less randomness\n",
        "                top_p=0.85,        # REDUCED: More focused sampling\n",
        "                pad_token_id=processor.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        inference_time = time.time() - inference_start\n",
        "        post_inference_memory = get_gpu_memory_usage()\n",
        "\n",
        "        # Decoding phase\n",
        "        decoding_start = time.time()\n",
        "        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n",
        "        result = decoded_output.split(\"ASSISTANT:\")[-1].strip() if \"ASSISTANT:\" in decoded_output else decoded_output\n",
        "        decoding_time = time.time() - decoding_start\n",
        "\n",
        "        # ENHANCED: Apply keyword filtering and prioritization\n",
        "        filtered_result = filter_llava_keywords(result, max_keywords=12)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Return results with timing data and filtering info\n",
        "        return {\n",
        "            'keywords': filtered_result,\n",
        "            'raw_keywords': result,  # Keep original for comparison\n",
        "            'timing': {\n",
        "                'image_load': image_load_time,\n",
        "                'processing': processing_time,\n",
        "                'inference': inference_time,\n",
        "                'decoding': decoding_time,\n",
        "                'total': total_time\n",
        "            },\n",
        "            'memory': {\n",
        "                'pre_inference': pre_inference_memory,\n",
        "                'post_inference': post_inference_memory,\n",
        "                'delta': post_inference_memory - pre_inference_memory\n",
        "            },\n",
        "            'filtering_applied': True\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'keywords': f'processing_failed: {str(e)}',\n",
        "            'timing': {'total': time.time() - start_time},\n",
        "            'memory': {'error': True},\n",
        "            'filtering_applied': False\n",
        "        }\n",
        "\n",
        "# Initialize processing variables\n",
        "specialist_results = []\n",
        "local_image_dir = \"_INPUT_IMAGES/\"\n",
        "processing_stats = {\n",
        "    'total_processing_time': 0,\n",
        "    'successful_inferences': 0,\n",
        "    'failed_inferences': 0,\n",
        "    'keyword_filtering_applied': 0,\n",
        "    'memory_peaks': [],\n",
        "    'avg_inference_time': 0\n",
        "}\n",
        "\n",
        "print(\"üöÄ Starting Enhanced Specialist Inference with Keyword Filtering\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "pipeline_timer.start_step(\"LLaVA Specialist Inference\")\n",
        "\n",
        "# ENHANCED: Try Business API first, then fall back to local images\n",
        "print(\"üì• PHASE 1: Image Acquisition\")\n",
        "dropbox_success, _ = download_dropbox_images_enhanced(image_folder_name, local_image_dir)\n",
        "\n",
        "if not dropbox_success:\n",
        "    print(\"‚ö†Ô∏è Dropbox Business API failed. Checking for local images...\")\n",
        "\n",
        "    # Check if local images already exist from previous runs\n",
        "    if os.path.exists(local_image_dir):\n",
        "        existing_images = [f for f in os.listdir(local_image_dir)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        if existing_images:\n",
        "            print(f\"‚úÖ Found {len(existing_images)} existing images in {local_image_dir}\")\n",
        "            print(\"üîÑ Continuing pipeline with existing images...\")\n",
        "        else:\n",
        "            print(f\"‚ùå No images found in {local_image_dir}. Cannot proceed.\")\n",
        "            print(\"üí° Please check Dropbox credentials or manually upload images to _INPUT_IMAGES/\")\n",
        "    else:\n",
        "        print(f\"‚ùå Directory {local_image_dir} does not exist. Cannot proceed.\")\n",
        "\n",
        "print(\"\\nü§ñ PHASE 2: Enhanced LLaVA Specialist Inference with Keyword Filtering\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "# Process images (from Dropbox or local) with enhanced keyword filtering\n",
        "if os.path.exists(local_image_dir):\n",
        "    image_files = [f for f in os.listdir(local_image_dir)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    if image_files:\n",
        "        print(f\"üéØ Processing {len(image_files)} images with enhanced keyword filtering\")\n",
        "        print(f\"üíæ Pre-processing GPU Memory: {get_gpu_memory_usage():.2f}GB\")\n",
        "        print(f\"üîß NEW: Keyword filtering, geographic correction, and redundancy removal\")\n",
        "\n",
        "        # Enhanced processing with comprehensive DEBUG logging\n",
        "        with tqdm(total=len(image_files), desc=\"üîÑ Enhanced LLaVA\", unit=\"img\", ncols=100) as pbar:\n",
        "            for i, filename in enumerate(image_files):\n",
        "                try:\n",
        "                    # Process image with enhanced filtering\n",
        "                    image_path = os.path.join(local_image_dir, filename)\n",
        "                    result = generate_keywords_with_finetuned_model_debug(image_path, model, processor)\n",
        "\n",
        "                    # Extract timing and memory data\n",
        "                    timing = result.get('timing', {})\n",
        "                    memory = result.get('memory', {})\n",
        "                    keywords = result.get('keywords', 'processing_failed')\n",
        "                    raw_keywords = result.get('raw_keywords', '')\n",
        "                    filtering_applied = result.get('filtering_applied', False)\n",
        "\n",
        "                    # Update statistics\n",
        "                    if 'processing_failed' not in keywords:\n",
        "                        processing_stats['successful_inferences'] += 1\n",
        "                        if filtering_applied:\n",
        "                            processing_stats['keyword_filtering_applied'] += 1\n",
        "                        if 'total' in timing:\n",
        "                            processing_stats['total_processing_time'] += timing['total']\n",
        "                    else:\n",
        "                        processing_stats['failed_inferences'] += 1\n",
        "\n",
        "                    # Track memory peaks\n",
        "                    if 'post_inference' in memory:\n",
        "                        processing_stats['memory_peaks'].append(memory['post_inference'])\n",
        "\n",
        "                    # Create detailed progress display\n",
        "                    progress_percent = ((i + 1) / len(image_files)) * 100\n",
        "                    total_time = timing.get('total', 0)\n",
        "                    current_memory = memory.get('post_inference', get_gpu_memory_usage())\n",
        "\n",
        "                    # Calculate keyword count for quality tracking\n",
        "                    keyword_count = len(keywords.split(',')) if keywords != 'processing_failed' else 0\n",
        "\n",
        "                    # Calculate ETA\n",
        "                    if processing_stats['successful_inferences'] > 0:\n",
        "                        avg_time = processing_stats['total_processing_time'] / processing_stats['successful_inferences']\n",
        "                        remaining_images = len(image_files) - (i + 1)\n",
        "                        eta_seconds = remaining_images * avg_time\n",
        "                        eta_display = f\"{eta_seconds:.1f}s\" if eta_seconds < 60 else f\"{eta_seconds/60:.1f}min\"\n",
        "                    else:\n",
        "                        eta_display = \"calculating...\"\n",
        "\n",
        "                    # Enhanced progress update with filtering info\n",
        "                    pbar.set_postfix_str(\n",
        "                        f\"‚è±Ô∏è{total_time:.2f}s | üñ•Ô∏è{current_memory:.1f}GB | üè∑Ô∏è{keyword_count}kw | üìä{progress_percent:.1f}% | ETA:{eta_display}\"\n",
        "                    )\n",
        "\n",
        "                    # Store result with enhanced metadata\n",
        "                    specialist_results.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"specialist_keywords\": keywords,\n",
        "                        \"raw_keywords\": raw_keywords,  # Store original for comparison\n",
        "                        \"filtering_applied\": filtering_applied,\n",
        "                        \"keyword_count\": keyword_count,\n",
        "                        \"debug_timing\": timing,\n",
        "                        \"debug_memory\": memory\n",
        "                    })\n",
        "\n",
        "                    # Memory cleanup every 5 images\n",
        "                    if (i + 1) % 5 == 0:\n",
        "                        cleanup_memory()\n",
        "                        pbar.set_postfix_str(f\"üßπ Memory cleanup | üñ•Ô∏è{get_gpu_memory_usage():.1f}GB\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to process {filename}: {e}\")\n",
        "                    # Add a placeholder entry to maintain data integrity\n",
        "                    specialist_results.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"specialist_keywords\": \"processing failed\",\n",
        "                        \"raw_keywords\": \"\",\n",
        "                        \"filtering_applied\": False,\n",
        "                        \"keyword_count\": 0,\n",
        "                        \"debug_error\": str(e)\n",
        "                    })\n",
        "                    processing_stats['failed_inferences'] += 1\n",
        "                    pbar.set_postfix_str(f\"‚ùå Error: {filename}\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Final cleanup and statistics\n",
        "        cleanup_memory()\n",
        "        final_memory = get_gpu_memory_usage()\n",
        "\n",
        "        # Calculate enhanced final statistics\n",
        "        total_images = len(specialist_results)\n",
        "        success_rate = (processing_stats['successful_inferences'] / total_images * 100) if total_images > 0 else 0\n",
        "        filtering_rate = (processing_stats['keyword_filtering_applied'] / processing_stats['successful_inferences'] * 100) if processing_stats['successful_inferences'] > 0 else 0\n",
        "        avg_processing_time = (processing_stats['total_processing_time'] / processing_stats['successful_inferences']) if processing_stats['successful_inferences'] > 0 else 0\n",
        "        peak_memory = max(processing_stats['memory_peaks']) if processing_stats['memory_peaks'] else 0\n",
        "\n",
        "        # Calculate average keyword count\n",
        "        successful_results = [r for r in specialist_results if r.get('keyword_count', 0) > 0]\n",
        "        avg_keyword_count = sum(r['keyword_count'] for r in successful_results) / len(successful_results) if successful_results else 0\n",
        "\n",
        "        pipeline_timer.end_step(\"LLaVA Specialist Inference\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Enhanced specialist inference complete!\")\n",
        "        print(f\"üìä ENHANCED PROCESSING STATISTICS:\")\n",
        "        print(f\"   üéØ Total Images: {total_images}\")\n",
        "        print(f\"   ‚úÖ Successful: {processing_stats['successful_inferences']} ({success_rate:.1f}%)\")\n",
        "        print(f\"   ‚ùå Failed: {processing_stats['failed_inferences']}\")\n",
        "        print(f\"   üîß Keyword Filtering Applied: {processing_stats['keyword_filtering_applied']} ({filtering_rate:.1f}%)\")\n",
        "        print(f\"   üè∑Ô∏è Average Keywords per Image: {avg_keyword_count:.1f}\")\n",
        "        print(f\"   ‚è±Ô∏è Avg Processing Time: {avg_processing_time:.2f}s/image\")\n",
        "        print(f\"   üñ•Ô∏è Peak GPU Memory: {peak_memory:.2f}GB\")\n",
        "        print(f\"   üíæ Final GPU Memory: {final_memory:.2f}GB\")\n",
        "        print(f\"   ‚è±Ô∏è Step duration: {pipeline_timer.get_step_duration('LLaVA Specialist Inference'):.1f} seconds\")\n",
        "\n",
        "        print(f\"üöÄ Ready for next pipeline step with enhanced keywords!\")\n",
        "\n",
        "\n",
        "        # OPTIONAL: Export LLaVA results to CSV for analysis\n",
        "        if specialist_results:\n",
        "            print(\"\\nüìä Exporting LLaVA results to CSV for analysis...\")\n",
        "            llava_csv_filename = \"llava_keywords_output.csv\"\n",
        "\n",
        "            try:\n",
        "                with open(llava_csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    fieldnames = ['filename', 'specialist_keywords', 'raw_keywords', 'keyword_count', 'filtering_applied']\n",
        "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                    writer.writeheader()\n",
        "\n",
        "                    for result in specialist_results:\n",
        "                        writer.writerow({\n",
        "                            'filename': result.get('filename', ''),\n",
        "                            'specialist_keywords': result.get('specialist_keywords', ''),\n",
        "                            'raw_keywords': result.get('raw_keywords', ''),\n",
        "                            'keyword_count': result.get('keyword_count', 0),\n",
        "                            'filtering_applied': result.get('filtering_applied', False)\n",
        "                        })\n",
        "\n",
        "                if os.path.exists(llava_csv_filename):\n",
        "                    file_size = os.path.getsize(llava_csv_filename)\n",
        "                    print(f\"‚úÖ LLaVA keywords exported to {llava_csv_filename} ({file_size:,} bytes)\")\n",
        "\n",
        "                    # Provide download link\n",
        "                    print(f\"\\n‚¨áÔ∏è DOWNLOADING LLaVA CSV RESULTS...\")\n",
        "                    bulletproof_download_csv(llava_csv_filename, \"LLaVA Keywords Output\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to create {llava_csv_filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå LLaVA CSV export failed: {e}\")\n",
        "                logging.error(f\"LLaVA CSV export error: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No specialist results to export\")\n",
        "\n",
        "        print(f\"üöÄ Ready for next pipeline step!\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ùå No image files found in {local_image_dir}\")\n",
        "else:\n",
        "    print(f\"‚ùå Image directory {local_image_dir} not found\")\n",
        "\n",
        "print(f\"\\nüéâ ENHANCED PROCESSING SUMMARY:\")\n",
        "print(f\"   ‚Ä¢ Dropbox Download: {'‚úÖ Success' if dropbox_success else '‚ùå Failed (using local)'}\")\n",
        "print(f\"   ‚Ä¢ Images Processed: {len(specialist_results)}\")\n",
        "print(f\"   ‚Ä¢ Keyword Filtering: {'‚úÖ Applied' if processing_stats['keyword_filtering_applied'] > 0 else '‚ùå Not applied'}\")\n",
        "print(f\"   ‚Ä¢ Geographic Correction: ‚úÖ Ready for GPT-4 phase\")\n",
        "print(f\"   ‚Ä¢ Ready for Enhanced GPT-4: {'‚úÖ Yes' if specialist_results else '‚ùå No'}\")"
      ],
      "metadata": {
        "id": "fn5MIrfrgcCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7_header"
      },
      "source": [
        "## Step 7: Save Intermediate JSON (WITH VALIDATION)\n",
        "Securely save specialist inference results with comprehensive validation and backup strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step7_save_intermediate"
      },
      "outputs": [],
      "source": [
        "print(\"üíæ PHASE 3: Intermediate Data Persistence with Enhanced Validation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pipeline_timer.start_step(\"JSON Save and Validation\")\n",
        "\n",
        "# Validate data before saving\n",
        "if validate_data_structure(specialist_results, \"Step 7 - Save Intermediate\"):\n",
        "    try:\n",
        "        # ENHANCED: Apply bulletproof escaping to all string data before JSON serialization\n",
        "        print(\"üîß Applying bulletproof string escaping to specialist results...\")\n",
        "        escaped_results = []\n",
        "\n",
        "        for item in specialist_results:\n",
        "            escaped_item = {}\n",
        "            for key, value in item.items():\n",
        "                if isinstance(value, str):\n",
        "                    # Apply bulletproof escaping to string values\n",
        "                    escaped_item[key] = bulletproof_escape_string(value)\n",
        "                elif isinstance(value, dict):\n",
        "                    # Handle nested dictionaries (like debug_timing, debug_memory)\n",
        "                    escaped_dict = {}\n",
        "                    for nested_key, nested_value in value.items():\n",
        "                        if isinstance(nested_value, str):\n",
        "                            escaped_dict[nested_key] = bulletproof_escape_string(nested_value)\n",
        "                        else:\n",
        "                            escaped_dict[nested_key] = nested_value\n",
        "                    escaped_item[key] = escaped_dict\n",
        "                else:\n",
        "                    escaped_item[key] = value\n",
        "            escaped_results.append(escaped_item)\n",
        "\n",
        "        print(f\"‚úÖ Bulletproof escaping applied to {len(escaped_results)} records\")\n",
        "\n",
        "        # ENHANCED: Progressive JSON validation before saving\n",
        "        print(\"üîç Performing progressive JSON validation...\")\n",
        "        validation_passed, validation_message = progressive_json_validation(\n",
        "            escaped_results,\n",
        "            \"Specialist Results - Pre-Save\"\n",
        "        )\n",
        "\n",
        "        if not validation_passed:\n",
        "            print(f\"‚ùå JSON validation failed: {validation_message}\")\n",
        "            print(\"üîÑ Attempting to save without escaping as fallback...\")\n",
        "            escaped_results = specialist_results\n",
        "        else:\n",
        "            print(\"‚úÖ Progressive JSON validation passed\")\n",
        "\n",
        "        # Primary save with enhanced error handling\n",
        "        primary_filename = 'intermediate_results.json'\n",
        "        print(f\"üíæ Saving primary intermediate file: {primary_filename}\")\n",
        "\n",
        "        with open(primary_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(escaped_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # ENHANCED: Post-save validation\n",
        "        print(\"üîç Performing post-save validation...\")\n",
        "        try:\n",
        "            with open(primary_filename, 'r', encoding='utf-8') as f:\n",
        "                test_load = json.load(f)\n",
        "\n",
        "            # Validate loaded data structure\n",
        "            post_save_validation, post_save_message = progressive_json_validation(\n",
        "                test_load,\n",
        "                \"Specialist Results - Post-Save\"\n",
        "            )\n",
        "\n",
        "            if len(test_load) == len(escaped_results) and post_save_validation:\n",
        "                print(f\"‚úÖ Primary save successful: {len(test_load)} records\")\n",
        "                print(f\"‚úÖ Post-save validation: {post_save_message}\")\n",
        "            else:\n",
        "                raise ValueError(f\"Data corruption detected: expected {len(escaped_results)}, got {len(test_load)} or validation failed: {post_save_message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Post-save validation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Create backup with timestamp and enhanced validation\n",
        "        import datetime\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        backup_filename = f'intermediate_results_backup_{timestamp}.json'\n",
        "\n",
        "        print(f\"üîÑ Creating timestamped backup: {backup_filename}\")\n",
        "        with open(backup_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(escaped_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Verify backup integrity\n",
        "        try:\n",
        "            with open(backup_filename, 'r', encoding='utf-8') as f:\n",
        "                backup_test = json.load(f)\n",
        "            backup_validation, backup_message = progressive_json_validation(\n",
        "                backup_test,\n",
        "                \"Backup File Validation\"\n",
        "            )\n",
        "            if len(backup_test) == len(escaped_results) and backup_validation:\n",
        "                print(f\"‚úÖ Backup validation successful: {backup_message}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Backup validation warning: {backup_message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Backup validation failed: {e}\")\n",
        "\n",
        "        # Generate enhanced save summary with quality metrics\n",
        "        total_keywords = 0\n",
        "        successful_saves = 0\n",
        "        escaped_string_count = 0\n",
        "\n",
        "        for item in escaped_results:\n",
        "            if 'specialist_keywords' in item and 'processing failed' not in item['specialist_keywords']:\n",
        "                successful_saves += 1\n",
        "                if isinstance(item['specialist_keywords'], str):\n",
        "                    total_keywords += len(item['specialist_keywords'].split(','))\n",
        "\n",
        "            # Count escaped strings for quality assurance\n",
        "            for key, value in item.items():\n",
        "                if isinstance(value, str) and ('\\\\' in value or '\"' in value):\n",
        "                    escaped_string_count += 1\n",
        "\n",
        "        pipeline_timer.end_step(\"JSON Save and Validation\")\n",
        "\n",
        "        print(f\"\\nüìä ENHANCED SAVE SUMMARY:\")\n",
        "        print(f\"   üìÅ Primary File: {primary_filename}\")\n",
        "        print(f\"   üîÑ Backup File: {backup_filename}\")\n",
        "        print(f\"   üìù Total Records: {len(escaped_results)}\")\n",
        "        print(f\"   ‚úÖ Successful Inferences: {successful_saves}\")\n",
        "        print(f\"   üè∑Ô∏è Total Keywords Generated: ~{total_keywords}\")\n",
        "        print(f\"   üõ°Ô∏è Strings Escaped: {escaped_string_count}\")\n",
        "        print(f\"   üîç JSON Validation: ‚úÖ Progressive validation applied\")\n",
        "        print(f\"   üíæ File Integrity: ‚úÖ Verified with post-save checks\")\n",
        "        print(f\"   ‚è±Ô∏è Save Duration: {pipeline_timer.get_step_duration('JSON Save and Validation'):.1f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Failed to save intermediate results: {e}\")\n",
        "        print(f\"‚ùå Enhanced save operation failed: {e}\")\n",
        "        print(\"üí° Specialist results remain in memory for next step\")\n",
        "        print(\"üîß Consider manual JSON export if critical\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No valid data to save from specialist inference.\")\n",
        "    print(\"üîÑ Pipeline will attempt to continue with available data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8_header"
      },
      "source": [
        "## Step 8: Load and Enrich Intermediate Data (WITH ERROR HANDLING)\n",
        "Load specialist results and enrich with photographer matching information using bulletproof error recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step8_load_enrich"
      },
      "outputs": [],
      "source": [
        "print(\"üîÑ PHASE 4: Data Loading and Enrichment with AI-Driven State Inference - NOW WITH BULLETPROOF ERROR HANDLING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "pipeline_timer.start_step(\"Data Enrichment\")\n",
        "\n",
        "# Multi-source data loading with fallback strategy\n",
        "intermediate_data = []\n",
        "data_source = \"unknown\"\n",
        "\n",
        "# Primary: Try loading from saved JSON file\n",
        "try:\n",
        "    print(\"üìÇ Attempting to load from intermediate_results.json...\")\n",
        "    with open('intermediate_results.json', 'r') as f:\n",
        "        intermediate_data = json.load(f)\n",
        "    data_source = \"intermediate_results.json\"\n",
        "    print(f\"‚úÖ Loaded {len(intermediate_data)} records from {data_source}\")\n",
        "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to load from file: {e}\")\n",
        "\n",
        "    # Fallback: Use in-memory specialist_results\n",
        "    if 'specialist_results' in globals() and specialist_results:\n",
        "        print(\"üîÑ Using in-memory specialist_results as fallback\")\n",
        "        intermediate_data = specialist_results\n",
        "        data_source = \"memory_fallback\"\n",
        "        print(f\"‚úÖ Loaded {len(intermediate_data)} records from memory\")\n",
        "    else:\n",
        "        print(\"‚ùå No fallback data available\")\n",
        "\n",
        "# BULLETPROOF: Validate loaded data with enhanced checks\n",
        "if not validate_data_structure(intermediate_data, \"Step 8 - Load\"):\n",
        "    print(\"‚ùå Critical error: No valid data available for enrichment\")\n",
        "    raise Exception(\"Pipeline cannot continue without specialist inference data\")\n",
        "\n",
        "print(f\"üë• Initializing photographer matching system...\")\n",
        "\n",
        "# BULLETPROOF: Initialize photographer matcher with comprehensive error handling\n",
        "matcher = None\n",
        "try:\n",
        "    matcher = PhotographerMatcher(PHOTOGRAPHER_CSV_PATH)\n",
        "    print(\"‚úÖ Photographer matching system ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Photographer matcher initialization failed: {e}\")\n",
        "    print(\"üîÑ Continuing with default photographer info\")\n",
        "    logging.error(f\"PhotographerMatcher init failed: {e}\")\n",
        "    matcher = None\n",
        "\n",
        "print(f\"üîÑ Enriching {len(intermediate_data)} records with BULLETPROOF error handling...\")\n",
        "\n",
        "# ENHANCED: Enrichment statistics with error details\n",
        "enriched_data = []\n",
        "enrichment_stats = {\n",
        "    'processed': 0,\n",
        "    'photographer_matches': 0,\n",
        "    'sku_extractions': 0,\n",
        "    'ai_state_inferences': 0,\n",
        "    'state_corrections': 0,\n",
        "    'errors': 0,\n",
        "    'critical_errors': 0,      # NEW: Track critical errors\n",
        "    'recoverable_errors': 0,   # NEW: Track recoverable errors\n",
        "    'vendor_info_failures': 0, # NEW: Track vendor info failures\n",
        "}\n",
        "\n",
        "# BULLETPROOF: Main enrichment loop with comprehensive error handling\n",
        "for item_index, item in enumerate(intermediate_data):\n",
        "    item_filename = \"unknown\"\n",
        "    try:\n",
        "        # DEFENSIVE: Validate item structure first\n",
        "        if not isinstance(item, dict):\n",
        "            logging.warning(f\"Item {item_index}: Not a dictionary, skipping: {type(item)}\")\n",
        "            enrichment_stats['errors'] += 1\n",
        "            continue\n",
        "\n",
        "        if 'filename' not in item:\n",
        "            logging.warning(f\"Item {item_index}: Missing filename field, skipping\")\n",
        "            enrichment_stats['errors'] += 1\n",
        "            continue\n",
        "\n",
        "        item_filename = item['filename']\n",
        "        logging.debug(f\"Processing item {item_index}: {item_filename}\")\n",
        "\n",
        "        # BULLETPROOF: Extract SKU from filename using multiple patterns with error handling\n",
        "        sku = ''\n",
        "        try:\n",
        "            filename = str(item_filename)\n",
        "            sku_patterns = [\n",
        "                r'\\b(\\d{6,})\\b',   # 6+ consecutive digits\n",
        "                r'\\b(\\d{4,})\\b',   # 4+ consecutive digits (fallback)\n",
        "                r'AH[_-]?(\\d+)',   # AH prefix pattern\n",
        "                r'([A-Z]{2}\\d+)'   # Letter-number combinations\n",
        "            ]\n",
        "\n",
        "            for pattern in sku_patterns:\n",
        "                try:\n",
        "                    sku_match = re.search(pattern, filename)\n",
        "                    if sku_match:\n",
        "                        sku = sku_match.group(1)\n",
        "                        enrichment_stats['sku_extractions'] += 1\n",
        "                        logging.debug(f\"SKU extracted for {item_filename}: {sku}\")\n",
        "                        break\n",
        "                except Exception as pattern_error:\n",
        "                    logging.warning(f\"Pattern matching error for {item_filename}: {pattern_error}\")\n",
        "                    continue\n",
        "\n",
        "        except Exception as sku_error:\n",
        "            logging.error(f\"SKU extraction failed for {item_filename}: {sku_error}\")\n",
        "            enrichment_stats['recoverable_errors'] += 1\n",
        "            sku = ''  # Continue with empty SKU\n",
        "\n",
        "        # BULLETPROOF: Get photographer/vendor information with comprehensive error handling\n",
        "        vendor_info = {'vendor': 'Unknown', 'photographer_match': 'no', 'state': 'Unknown'}\n",
        "        try:\n",
        "            if matcher and sku:\n",
        "                # CRITICAL: This is where the tuple unpacking error was occurring - NOW FIXED!\n",
        "                try:\n",
        "                    vendor_info = matcher.get_vendor_info(sku)\n",
        "\n",
        "                    # DEFENSIVE: Validate return value structure\n",
        "                    if not isinstance(vendor_info, dict):\n",
        "                        logging.warning(f\"Invalid vendor_info type for {item_filename}: {type(vendor_info)}\")\n",
        "                        vendor_info = {'vendor': 'Unknown', 'photographer_match': 'no', 'state': 'Unknown'}\n",
        "                        enrichment_stats['vendor_info_failures'] += 1\n",
        "                    else:\n",
        "                        # Ensure all required keys exist\n",
        "                        required_keys = ['vendor', 'photographer_match', 'state']\n",
        "                        for key in required_keys:\n",
        "                            if key not in vendor_info:\n",
        "                                vendor_info[key] = 'Unknown' if key != 'photographer_match' else 'no'\n",
        "\n",
        "                        if vendor_info.get('photographer_match') == 'yes':\n",
        "                            enrichment_stats['photographer_matches'] += 1\n",
        "                            logging.debug(f\"Photographer match found for {item_filename}: {vendor_info['vendor']}\")\n",
        "\n",
        "                except Exception as vendor_error:\n",
        "                    logging.error(f\"Vendor info retrieval failed for {item_filename} (SKU: {sku}): {vendor_error}\")\n",
        "                    enrichment_stats['vendor_info_failures'] += 1\n",
        "                    enrichment_stats['recoverable_errors'] += 1\n",
        "                    # vendor_info already has default values\n",
        "\n",
        "            else:\n",
        "                # DEFENSIVE: Provide defaults if no matcher or SKU\n",
        "                if not matcher:\n",
        "                    logging.debug(f\"No matcher available for {item_filename}\")\n",
        "                if not sku:\n",
        "                    logging.debug(f\"No SKU available for {item_filename}\")\n",
        "\n",
        "        except Exception as vendor_outer_error:\n",
        "            logging.error(f\"Critical vendor processing error for {item_filename}: {vendor_outer_error}\")\n",
        "            enrichment_stats['critical_errors'] += 1\n",
        "            # vendor_info already has default values\n",
        "\n",
        "        # BULLETPROOF: AI-Driven State Inference with enhanced error handling\n",
        "        try:\n",
        "            original_state = vendor_info.get('state', 'Unknown')\n",
        "            ai_generated_keywords = item.get('specialist_keywords', '')\n",
        "\n",
        "            if ai_generated_keywords and ai_generated_keywords != 'processing failed':\n",
        "                try:\n",
        "                    # Use AI-driven state inference\n",
        "                    photographer_context = {\n",
        "                        'state': original_state,\n",
        "                        'photographer': vendor_info.get('vendor', 'Unknown')\n",
        "                    }\n",
        "\n",
        "                    inferred_state = infer_photographer_state(ai_generated_keywords, photographer_context)\n",
        "\n",
        "                    # DEFENSIVE: Validate inferred state\n",
        "                    if not isinstance(inferred_state, str):\n",
        "                        inferred_state = str(inferred_state) if inferred_state is not None else 'Unknown'\n",
        "\n",
        "                    # Track AI inference usage\n",
        "                    if inferred_state != original_state:\n",
        "                        enrichment_stats['ai_state_inferences'] += 1\n",
        "                        if original_state == 'Unknown' or not original_state:\n",
        "                            logging.info(f\"AI inferred state '{inferred_state}' for {item_filename}\")\n",
        "                        else:\n",
        "                            enrichment_stats['state_corrections'] += 1\n",
        "                            logging.info(f\"AI corrected state from '{original_state}' to '{inferred_state}' for {item_filename}\")\n",
        "\n",
        "                    # Update vendor info with AI-inferred state\n",
        "                    vendor_info['state'] = inferred_state\n",
        "                    vendor_info['ai_state_inference'] = 'yes' if inferred_state != original_state else 'no'\n",
        "\n",
        "                except Exception as ai_inference_error:\n",
        "                    logging.error(f\"AI state inference failed for {item_filename}: {ai_inference_error}\")\n",
        "                    vendor_info['ai_state_inference'] = 'no'\n",
        "                    enrichment_stats['recoverable_errors'] += 1\n",
        "            else:\n",
        "                vendor_info['ai_state_inference'] = 'no'  # Mark as no AI inference used\n",
        "\n",
        "        except Exception as ai_outer_error:\n",
        "            logging.error(f\"AI processing section failed for {item_filename}: {ai_outer_error}\")\n",
        "            vendor_info['ai_state_inference'] = 'no'\n",
        "            enrichment_stats['critical_errors'] += 1\n",
        "\n",
        "        # BULLETPROOF: Enrich the item with all gathered data\n",
        "        try:\n",
        "            # DEFENSIVE: Ensure item is still a dictionary\n",
        "            if not isinstance(item, dict):\n",
        "                logging.error(f\"Item became non-dict during processing: {item_filename}\")\n",
        "                item = {'filename': item_filename}  # Reconstruct basic item\n",
        "\n",
        "            item['sku'] = sku if sku else ''\n",
        "\n",
        "            # DEFENSIVE: Update item with vendor_info safely\n",
        "            if isinstance(vendor_info, dict):\n",
        "                item.update(vendor_info)\n",
        "            else:\n",
        "                logging.warning(f\"vendor_info is not dict for {item_filename}, adding defaults\")\n",
        "                item.update({'vendor': 'Unknown', 'photographer_match': 'no', 'state': 'Unknown', 'ai_state_inference': 'no'})\n",
        "\n",
        "            enriched_data.append(item)\n",
        "            enrichment_stats['processed'] += 1\n",
        "\n",
        "            # Progress logging for large datasets\n",
        "            if (item_index + 1) % 100 == 0:\n",
        "                logging.info(f\"Processed {item_index + 1}/{len(intermediate_data)} items ({enrichment_stats['processed']} successful)\")\n",
        "\n",
        "        except Exception as enrichment_error:\n",
        "            logging.error(f\"Final enrichment failed for {item_filename}: {enrichment_error}\")\n",
        "            # DEFENSIVE: Add item with minimal enrichment to maintain data flow\n",
        "            try:\n",
        "                minimal_item = {\n",
        "                    'filename': item_filename,\n",
        "                    'sku': '',\n",
        "                    'vendor': 'Unknown',\n",
        "                    'photographer_match': 'no',\n",
        "                    'state': 'Unknown',\n",
        "                    'ai_state_inference': 'no',\n",
        "                    'enrichment_error': str(enrichment_error)\n",
        "                }\n",
        "                # Try to preserve original item data\n",
        "                if isinstance(item, dict):\n",
        "                    for key, value in item.items():\n",
        "                        if key not in minimal_item:\n",
        "                            minimal_item[key] = value\n",
        "\n",
        "                enriched_data.append(minimal_item)\n",
        "                enrichment_stats['critical_errors'] += 1\n",
        "            except Exception as minimal_error:\n",
        "                logging.critical(f\"Could not create minimal item for {item_filename}: {minimal_error}\")\n",
        "                enrichment_stats['critical_errors'] += 1\n",
        "\n",
        "    except Exception as outer_error:\n",
        "        # CRITICAL: Outer exception handler for completely unexpected errors\n",
        "        logging.critical(f\"Catastrophic error processing item {item_index} ({item_filename}): {outer_error}\")\n",
        "        enrichment_stats['critical_errors'] += 1\n",
        "\n",
        "        # DEFENSIVE: Try to add a basic error item\n",
        "        try:\n",
        "            error_item = {\n",
        "                'filename': item_filename,\n",
        "                'sku': '',\n",
        "                'vendor': 'Unknown',\n",
        "                'photographer_match': 'no',\n",
        "                'state': 'Unknown',\n",
        "                'ai_state_inference': 'no',\n",
        "                'critical_error': str(outer_error)\n",
        "            }\n",
        "            enriched_data.append(error_item)\n",
        "        except:\n",
        "            logging.critical(f\"Could not create error item for {item_filename}\")\n",
        "\n",
        "pipeline_timer.end_step(\"Data Enrichment\")\n",
        "\n",
        "# ENHANCED: Validation and comprehensive summary with error details\n",
        "if validate_data_structure(enriched_data, \"Step 8 - Enrich\"):\n",
        "    total_items = len(intermediate_data) if intermediate_data else 0\n",
        "    success_rate = (enrichment_stats['processed'] / total_items * 100) if total_items > 0 else 0\n",
        "    match_rate = (enrichment_stats['photographer_matches'] / enrichment_stats['processed'] * 100) if enrichment_stats['processed'] > 0 else 0\n",
        "    ai_inference_rate = (enrichment_stats['ai_state_inferences'] / enrichment_stats['processed'] * 100) if enrichment_stats['processed'] > 0 else 0\n",
        "    error_rate = ((enrichment_stats['errors'] + enrichment_stats['critical_errors'] + enrichment_stats['recoverable_errors']) / total_items * 100) if total_items > 0 else 0\n",
        "\n",
        "    print(f\"‚úÖ BULLETPROOF data enrichment complete!\")\n",
        "    print(f\"üìä COMPREHENSIVE ENRICHMENT STATISTICS:\")\n",
        "    print(f\"   üìÇ Data Source: {data_source}\")\n",
        "    print(f\"   üìù Total Input Records: {total_items}\")\n",
        "    print(f\"   üìä Total Output Records: {len(enriched_data)}\")\n",
        "    print(f\"   ‚úÖ Successfully Processed: {enrichment_stats['processed']} ({success_rate:.1f}%)\")\n",
        "    print(f\"   üè∑Ô∏è SKU Extractions: {enrichment_stats['sku_extractions']}\")\n",
        "    print(f\"   üë• Photographer Matches: {enrichment_stats['photographer_matches']} ({match_rate:.1f}%)\")\n",
        "    print(f\"   ü§ñ AI State Inferences: {enrichment_stats['ai_state_inferences']} ({ai_inference_rate:.1f}%)\")\n",
        "    print(f\"   üîß State Corrections: {enrichment_stats['state_corrections']}\")\n",
        "    print(f\"   ‚ö†Ô∏è Total Error Rate: {error_rate:.1f}%\")\n",
        "    print(f\"   üõ°Ô∏è ERROR BREAKDOWN:\")\n",
        "    print(f\"      ‚Ä¢ General Errors: {enrichment_stats['errors']}\")\n",
        "    print(f\"      ‚Ä¢ Critical Errors: {enrichment_stats['critical_errors']}\")\n",
        "    print(f\"      ‚Ä¢ Recoverable Errors: {enrichment_stats['recoverable_errors']}\")\n",
        "    print(f\"      ‚Ä¢ Vendor Info Failures: {enrichment_stats['vendor_info_failures']}\")\n",
        "    print(f\"   ‚è±Ô∏è Enrichment Duration: {pipeline_timer.get_step_duration('Data Enrichment'):.1f} seconds\")\n",
        "    print(f\"   üöÄ Ready for GPT-4 Vision refinement!\")\n",
        "\n",
        "    # ENHANCED: Log summary for debugging\n",
        "    logging.info(f\"Enrichment completed: {enrichment_stats['processed']}/{total_items} successful ({success_rate:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Failed to enrich data properly.\")\n",
        "    print(f\"‚ö†Ô∏è ERROR SUMMARY: {enrichment_stats}\")\n",
        "    raise Exception(\"Data enrichment failed - cannot proceed to GPT-4 refinement\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîß AGStock Keyworder Enhanced v4.0 FIXED - GPT-4 Vision Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è OpenAI API key not configured\")\n",
        "    print(\"üí° GPT-4 Vision refinement will use enhanced fallback responses\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key configured for GPT-4 Vision\")\n",
        "    print(\"ü§ñ Using gpt-4o model with geographic correction\")\n",
        "\n",
        "print(\"üåç Geographic correction system ready\")\n",
        "print(\"üìè Strict format compliance (8-12 keywords) enforced\")\n",
        "print(\"üõ°Ô∏è Enhanced rate limiting and cost optimization active\")\n",
        "print(\"üîß Enhanced fallback responses with agricultural intelligence\")"
      ],
      "metadata": {
        "id": "ZyI3mNPLgcCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9_header"
      },
      "source": [
        "## Step 9: üî• Enhanced GPT-4 Vision Refinement with Geographic Correction\n",
        "Production-grade GPT-4 Vision processing with enhanced keyword filtering, geographic correction, and strict format compliance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIxZPUqygcCQ"
      },
      "source": [
        "## Step 9: Enhanced GPT-4 Vision Refinement with Geographic Correction\n",
        "\n",
        "This step implements:\n",
        "- API rate limiting with intelligent throttling\n",
        "- Enhanced GPT-4o Vision calls with 4-step structured prompts\n",
        "- Geographic correction based on photographer context\n",
        "- Comprehensive error handling and fallback mechanisms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt4_vision_setup_fixed"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"AGStock Keyworder Enhanced v4.0 FIXED - GPT-4 Vision Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"OpenAI API key not configured\")\n",
        "    print(\"GPT-4 Vision refinement will use fallback mode\")\n",
        "    client = None\n",
        "else:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"OpenAI GPT-4 Vision client initialized\")\n",
        "\n",
        "print(\"\\nStep 9 Ready: Enhanced GPT-4o Refinement with Rate Limiting\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step9_rate_limiter"
      },
      "outputs": [],
      "source": [
        "# Step 9 Part 1: Rate Limiter Class\n",
        "\n",
        "class SimpleRateLimiter:\n",
        "    \"\"\"Simple rate limiter for API requests\"\"\"\n",
        "\n",
        "    def __init__(self, requests_per_minute=30):\n",
        "        self.requests_per_minute = requests_per_minute\n",
        "        self.request_times = []\n",
        "        self.throttle_events = 0\n",
        "        self.total_throttle_delay = 0.0\n",
        "\n",
        "    def wait_if_needed(self):\n",
        "        \"\"\"Simple rate limiting check\"\"\"\n",
        "        now = time.time()\n",
        "\n",
        "        # Remove requests older than 1 minute\n",
        "        self.request_times = [t for t in self.request_times if now - t < 60]\n",
        "\n",
        "        # Check if we need to wait\n",
        "        if len(self.request_times) >= self.requests_per_minute:\n",
        "            sleep_time = 60 - (now - self.request_times[0])\n",
        "            if sleep_time > 0:\n",
        "                logging.info(f\"Rate limiting: waiting {sleep_time:.1f}s\")\n",
        "                self.throttle_events += 1\n",
        "                self.total_throttle_delay += sleep_time\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "        self.request_times.append(now)\n",
        "\n",
        "    def get_rate_limit_status(self):\n",
        "        \"\"\"Get current rate limiting status\"\"\"\n",
        "        now = time.time()\n",
        "        recent_requests = len([t for t in self.request_times if now - t < 60])\n",
        "        hour_requests = len([t for t in self.request_times if now - t < 3600])\n",
        "\n",
        "        # Calculate estimated cost (approximation: $0.01 per 1K tokens, assume ~500 tokens per request)\n",
        "        estimated_cost = len(self.request_times) * 0.005\n",
        "\n",
        "        # Calculate cost savings from throttling (approximate)\n",
        "        cost_savings = self.throttle_events * 0.001  # Small savings from avoiding rate limit penalties\n",
        "\n",
        "        # Calculate average throttle delay\n",
        "        avg_throttle_delay = (self.total_throttle_delay / self.throttle_events) if self.throttle_events > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            'current_minute_usage': f\"{recent_requests}/{self.requests_per_minute}\",\n",
        "            'current_hour_usage': f\"{hour_requests}/{self.requests_per_minute * 60}\",\n",
        "            'total_requests': len(self.request_times),\n",
        "            'throttle_events': self.throttle_events,\n",
        "            'estimated_cost': f\"${estimated_cost:.3f}\",\n",
        "            'cost_savings': f\"${cost_savings:.3f}\",\n",
        "            'avg_throttle_delay': f\"{avg_throttle_delay:.2f}s\"\n",
        "        }\n",
        "\n",
        "# Initialize rate limiter\n",
        "api_rate_limiter = SimpleRateLimiter(requests_per_minute=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step9_helper_functions"
      },
      "outputs": [],
      "source": [
        "# Step 9 Part 2: Helper Functions\n",
        "\n",
        "def create_enhanced_fallback_response(specialist_keywords, ground_truth):\n",
        "    \"\"\"Create high-quality fallback response with geographic correction and format compliance\"\"\"\n",
        "\n",
        "    # Clean and enhance specialist keywords\n",
        "    if specialist_keywords and specialist_keywords != 'processing failed':\n",
        "        # Apply the same filtering functions used in LLaVA processing\n",
        "        filtered_keywords = filter_llava_keywords(specialist_keywords, max_keywords=12)\n",
        "        cleaned_keywords = [kw.strip() for kw in filtered_keywords.split(',')]\n",
        "    else:\n",
        "        cleaned_keywords = ['agriculture', 'farming']\n",
        "\n",
        "    # Apply geographic correction based on photographer context\n",
        "    photographer_context = {\n",
        "        'state': ground_truth.get('location', 'Unknown'),\n",
        "        'photographer': ground_truth.get('photographer', 'Unknown')\n",
        "    }\n",
        "\n",
        "    # Join keywords for geographic correction\n",
        "    keywords_string = ', '.join(cleaned_keywords)\n",
        "    corrected_keywords = correct_geographic_keywords(keywords_string, photographer_context)\n",
        "\n",
        "    # Apply redundancy removal\n",
        "    final_keyword_list = [kw.strip() for kw in corrected_keywords.split(',')]\n",
        "    final_keywords = remove_redundant_terms(final_keyword_list)\n",
        "\n",
        "    # Ensure 8-12 keyword count\n",
        "    if len(final_keywords) < 8:\n",
        "        base_keywords = ['agriculture', 'farming', 'crop', 'field', 'rural', 'farm']\n",
        "        for kw in base_keywords:\n",
        "            if kw not in final_keywords and len(final_keywords) < 8:\n",
        "                final_keywords.append(kw)\n",
        "    elif len(final_keywords) > 12:\n",
        "        final_keywords = final_keywords[:12]\n",
        "\n",
        "    # Generate title with geographic context\n",
        "    location = ground_truth.get('location', 'Unknown')\n",
        "    if location and location != 'Unknown':\n",
        "        title = f\"Agricultural Image {location}\"\n",
        "    else:\n",
        "        title = \"Agricultural Image\"\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"keywords\": ', '.join(final_keywords),\n",
        "        \"debug_info\": {\n",
        "            \"fallback_used\": True,\n",
        "            \"fallback_type\": \"enhanced_with_geographic_correction\",\n",
        "            \"geographic_correction_applied\": True,\n",
        "            \"success\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "def validate_gpt4_response(result):\n",
        "    \"\"\"Enhanced validation for GPT-4 response quality and format compliance\"\"\"\n",
        "    if not result or not isinstance(result, dict):\n",
        "        return False, \"Invalid response format\"\n",
        "\n",
        "    if 'title' not in result or 'keywords' not in result:\n",
        "        return False, \"Missing required fields\"\n",
        "\n",
        "    # Validate title\n",
        "    title = result['title'].strip()\n",
        "    if not title or len(title.split()) > 10:\n",
        "        return False, \"Title too long or empty (max 10 words)\"\n",
        "\n",
        "    # Enhanced keyword validation\n",
        "    keywords = result['keywords'].strip()\n",
        "    if not keywords:\n",
        "        return False, \"Keywords empty\"\n",
        "\n",
        "    keyword_list = [k.strip() for k in keywords.split(',')]\n",
        "    keyword_list = [k for k in keyword_list if k]  # Remove empty keywords\n",
        "\n",
        "    if len(keyword_list) < 8:\n",
        "        return False, f\"Too few keywords ({len(keyword_list)} found, minimum 8 required)\"\n",
        "    elif len(keyword_list) > 12:\n",
        "        return False, f\"Too many keywords ({len(keyword_list)} found, maximum 12 allowed)\"\n",
        "\n",
        "    return True, \"Valid response\"\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step9_main_refinement"
      },
      "outputs": [],
      "source": [
        "# Step 9 Part 3: GPT-4 Vision Processing Function\n",
        "\n",
        "# API call function with retry logic\n",
        "@simple_retry(max_attempts=3, delay=2, backoff=2)\n",
        "def make_api_call(url, headers, data):\n",
        "    \"\"\"Simplified API call with automatic retry\"\"\"\n",
        "    response = requests.post(url, headers=headers, json=data, timeout=45)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def refine_with_enhanced_gpt4_vision(image_path, specialist_keywords, ground_truth):\n",
        "    \"\"\"GPT-4o Vision refinement with enhanced processing\"\"\"\n",
        "    start_time = time.time()\n",
        "    debug_info = {\n",
        "        'success': False,\n",
        "        'fallback_used': False,\n",
        "        'model_used': 'gpt-4o',\n",
        "        'simplified_retry': True\n",
        "    }\n",
        "\n",
        "    if not OPENAI_API_KEY:\n",
        "        debug_info['error'] = \"No OpenAI API key provided\"\n",
        "        return create_enhanced_fallback_response(specialist_keywords, ground_truth)\n",
        "\n",
        "    # Simple rate limiting\n",
        "    api_rate_limiter.wait_if_needed()\n",
        "\n",
        "    # Image encoding\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            base64_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        debug_info['error'] = f\"Image encoding failed: {str(e)}\"\n",
        "        return create_enhanced_fallback_response(specialist_keywords, ground_truth)\n",
        "\n",
        "    # API request setup\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"}\n",
        "\n",
        "    prompt_text = f\"\"\"You are an expert agricultural photography keywording system.\n",
        "\n",
        "**CONTEXT:**\n",
        "- LLaVA Keywords: {specialist_keywords}\n",
        "- Photographer: {ground_truth.get('photographer', 'Unknown')}\n",
        "- Location: {ground_truth.get('location', 'Unknown')}\n",
        "\n",
        "**REQUIREMENTS:**\n",
        "1. Use ONLY the confirmed location\n",
        "2. Generate EXACTLY 8-12 keywords\n",
        "3. Output JSON format only\n",
        "\n",
        "**OUTPUT FORMAT:**\n",
        "```json\n",
        "{{\n",
        "  \"title\": \"[SEO title with location, max 8 words]\",\n",
        "  \"keywords\": \"[8-12 comma-separated lowercase keywords]\"\n",
        "}}\n",
        "```\n",
        "\n",
        "Enhance the keywords following this format.\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"gpt-4o\",\n",
        "        \"messages\": [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": prompt_text},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
        "            ]\n",
        "        }],\n",
        "        \"max_tokens\": 400,\n",
        "        \"temperature\": 0.2\n",
        "    }\n",
        "\n",
        "    # API call with automatic retry\n",
        "    try:\n",
        "        response = make_api_call(\"https://api.openai.com/v1/chat/completions\", headers, payload)\n",
        "        content = response['choices'][0]['message']['content']\n",
        "        debug_info['api_success'] = True\n",
        "    except Exception as e:\n",
        "        debug_info['api_success'] = False\n",
        "        debug_info['api_error'] = str(e)\n",
        "        debug_info['fallback_used'] = True\n",
        "        return create_enhanced_fallback_response(specialist_keywords, ground_truth)\n",
        "\n",
        "    # Response parsing\n",
        "    try:\n",
        "        # Try JSON code block extraction first\n",
        "        json_match = re.search(r'```(?:json)?\\n(.*?)\\n```', content, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group(1).strip()\n",
        "            parsed_response = json.loads(json_str)\n",
        "        else:\n",
        "            # Try direct JSON parsing\n",
        "            clean_content = content.strip()\n",
        "            if clean_content.startswith('```'):\n",
        "                clean_content = re.sub(r'^```.*?\\n', '', clean_content)\n",
        "                clean_content = re.sub(r'\\n```$', '', clean_content)\n",
        "            parsed_response = json.loads(clean_content)\n",
        "\n",
        "        if isinstance(parsed_response, dict) and 'keywords' in parsed_response:\n",
        "            # Apply geographic correction and keyword validation\n",
        "            photographer_context = {\n",
        "                'state': ground_truth.get('location', 'Unknown'),\n",
        "                'photographer': ground_truth.get('photographer', 'Unknown')\n",
        "            }\n",
        "\n",
        "            original_keywords = parsed_response.get('keywords', '')\n",
        "            corrected_keywords = correct_geographic_keywords(original_keywords, photographer_context)\n",
        "            keyword_list = [kw.strip() for kw in corrected_keywords.split(',')]\n",
        "            final_keywords = remove_redundant_terms(keyword_list)\n",
        "\n",
        "            # Ensure 8-12 keyword compliance\n",
        "            if len(final_keywords) < 8:\n",
        "                base_keywords = ['agriculture', 'farming', 'crop', 'field']\n",
        "                for kw in base_keywords:\n",
        "                    if kw not in final_keywords and len(final_keywords) < 8:\n",
        "                        final_keywords.append(kw)\n",
        "            elif len(final_keywords) > 12:\n",
        "                final_keywords = final_keywords[:12]\n",
        "\n",
        "            debug_info['success'] = True\n",
        "            result = {\n",
        "                \"title\": parsed_response.get('title', 'Agricultural Image'),\n",
        "                \"keywords\": ', '.join(final_keywords),\n",
        "                \"debug_info\": debug_info\n",
        "            }\n",
        "\n",
        "            # Validate result\n",
        "            is_valid, validation_message = validate_gpt4_response(result)\n",
        "            if is_valid:\n",
        "                return result\n",
        "            else:\n",
        "                debug_info['validation_error'] = validation_message\n",
        "\n",
        "    except (json.JSONDecodeError, KeyError) as e:\n",
        "        debug_info['parsing_error'] = str(e)\n",
        "\n",
        "    # Final fallback\n",
        "    debug_info['fallback_used'] = True\n",
        "    return create_enhanced_fallback_response(specialist_keywords, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üéØ PHASE 5: Enhanced GPT-4o Vision Refinement with Geographic Correction\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "pipeline_timer.start_step(\"Enhanced GPT-4o Refinement\")\n",
        "\n",
        "# Initialize enhanced refinement tracking\n",
        "refined_data = []\n",
        "enhanced_gpt4_stats = {\n",
        "    'total_processed': 0,\n",
        "    'api_successes': 0,\n",
        "    'api_failures': 0,\n",
        "    'parsing_successes': 0,\n",
        "    'fallback_used': 0,\n",
        "    'model_retries': 0,\n",
        "    'total_api_time': 0,\n",
        "    'total_parsing_time': 0,\n",
        "    'rate_limit_events': 0,\n",
        "    'geographic_corrections_applied': 0,\n",
        "    'format_compliance_enforced': 0,\n",
        "    'validation_passes': 0,\n",
        "    'avg_keywords_per_image': 0\n",
        "}\n",
        "\n",
        "if validate_data_structure(enriched_data, \"Step 9 - Enhanced Input\"):\n",
        "    print(f\"üöÄ Processing {len(enriched_data)} images with Enhanced GPT-4o Vision\")\n",
        "    print(f\"üîß Using enhanced geographic correction and strict format compliance\")\n",
        "    print(f\"ü§ñ Model: gpt-4o with keyword filtering integration\")\n",
        "    print(f\"üõ°Ô∏è Intelligent rate limiting: {api_rate_limiter.requests_per_minute} req/min\")\n",
        "    print(f\"üåç Geographic correction: Enabled for photographer context\")\n",
        "    print(f\"üìä Format compliance: Strict 8-12 keyword enforcement\")\n",
        "\n",
        "    total_keywords_generated = 0\n",
        "\n",
        "    with tqdm(total=len(enriched_data), desc=\"üéØ Enhanced GPT-4o\", unit=\"img\", ncols=120) as pbar:\n",
        "        for i, item in enumerate(enriched_data):\n",
        "            try:\n",
        "                image_path = os.path.join(local_image_dir, item['filename'])\n",
        "\n",
        "                # Validate image file exists\n",
        "                if not os.path.exists(image_path):\n",
        "                    item.update({\"title\": \"Image Not Found\", \"keywords\": item.get('specialist_keywords', '')})\n",
        "                    enhanced_gpt4_stats['api_failures'] += 1\n",
        "                else:\n",
        "                    ground_truth = {\n",
        "                        'photographer': item.get('vendor', 'Unknown'),\n",
        "                        'location': item.get('state', 'Unknown')\n",
        "                    }\n",
        "\n",
        "                    # Process with enhanced GPT-4o with geographic correction\n",
        "                    gpt4_output = refine_with_enhanced_gpt4_vision(\n",
        "                        image_path,\n",
        "                        item.get('specialist_keywords', ''),\n",
        "                        ground_truth\n",
        "                    )\n",
        "\n",
        "                    # Extract enhanced debug information\n",
        "                    debug_info = gpt4_output.get('debug_info', {})\n",
        "\n",
        "                    # Update enhanced statistics\n",
        "                    if debug_info.get('api_success', False):\n",
        "                        enhanced_gpt4_stats['api_successes'] += 1\n",
        "                    else:\n",
        "                        enhanced_gpt4_stats['api_failures'] += 1\n",
        "\n",
        "                    if debug_info.get('success', False):\n",
        "                        enhanced_gpt4_stats['parsing_successes'] += 1\n",
        "\n",
        "                    if debug_info.get('fallback_used', False):\n",
        "                        enhanced_gpt4_stats['fallback_used'] += 1\n",
        "\n",
        "                    if debug_info.get('geographic_correction_applied', False):\n",
        "                        enhanced_gpt4_stats['geographic_corrections_applied'] += 1\n",
        "\n",
        "                    if debug_info.get('format_compliance_enforced', False):\n",
        "                        enhanced_gpt4_stats['format_compliance_enforced'] += 1\n",
        "\n",
        "                    # Count keywords for quality tracking\n",
        "                    final_keywords = gpt4_output.get('keywords', '')\n",
        "                    keyword_count = len(final_keywords.split(',')) if final_keywords else 0\n",
        "                    total_keywords_generated += keyword_count\n",
        "\n",
        "                    # Validate final output\n",
        "                    is_valid, validation_message = validate_gpt4_response(gpt4_output)\n",
        "                    if is_valid:\n",
        "                        enhanced_gpt4_stats['validation_passes'] += 1\n",
        "\n",
        "                    if debug_info.get('api_attempts', 0) > 1:\n",
        "                        enhanced_gpt4_stats['model_retries'] += debug_info['api_attempts'] - 1\n",
        "\n",
        "                    # Track rate limiting events\n",
        "                    rate_limiting_info = debug_info.get('rate_limiting', {})\n",
        "                    if rate_limiting_info.get('throttle_applied', False):\n",
        "                        enhanced_gpt4_stats['rate_limit_events'] += 1\n",
        "\n",
        "                    # Track enhanced timing\n",
        "                    phases = debug_info.get('phases', {})\n",
        "                    if 'api_call' in phases:\n",
        "                        enhanced_gpt4_stats['total_api_time'] += phases['api_call']\n",
        "                    if 'parsing' in phases:\n",
        "                        enhanced_gpt4_stats['total_parsing_time'] += phases['parsing']\n",
        "\n",
        "                    # Update item with enhanced results\n",
        "                    item.update({\n",
        "                        \"title\": gpt4_output.get('title', 'Processing Failed'),\n",
        "                        \"keywords\": final_keywords,\n",
        "                        \"enhanced_gpt4_debug\": debug_info,\n",
        "                        \"model_version\": \"gpt-4o-enhanced\",\n",
        "                        \"keyword_count\": keyword_count,\n",
        "                        \"geographic_corrected\": debug_info.get('geographic_correction_applied', False),\n",
        "                        \"format_compliant\": debug_info.get('format_compliance_enforced', False)\n",
        "                    })\n",
        "\n",
        "                refined_data.append(item)\n",
        "                enhanced_gpt4_stats['total_processed'] += 1\n",
        "\n",
        "                # Calculate enhanced progress statistics\n",
        "                progress_percent = ((i + 1) / len(enriched_data)) * 100\n",
        "                success_rate = (enhanced_gpt4_stats['api_successes'] / enhanced_gpt4_stats['total_processed'] * 100) if enhanced_gpt4_stats['total_processed'] > 0 else 0\n",
        "                validation_rate = (enhanced_gpt4_stats['validation_passes'] / enhanced_gpt4_stats['total_processed'] * 100) if enhanced_gpt4_stats['total_processed'] > 0 else 0\n",
        "\n",
        "                # Get current rate limit status for display\n",
        "                rate_status = api_rate_limiter.get_rate_limit_status()\n",
        "\n",
        "                # Calculate ETA with enhanced metrics\n",
        "                if enhanced_gpt4_stats['total_processed'] > 0:\n",
        "                    avg_time = (enhanced_gpt4_stats['total_api_time'] + enhanced_gpt4_stats['total_parsing_time']) / enhanced_gpt4_stats['total_processed']\n",
        "                    remaining_items = len(enriched_data) - (i + 1)\n",
        "                    eta_seconds = remaining_items * avg_time\n",
        "                    eta_display = f\"{eta_seconds:.1f}s\" if eta_seconds < 60 else f\"{eta_seconds/60:.1f}min\"\n",
        "                else:\n",
        "                    eta_display = \"calculating...\"\n",
        "\n",
        "                # Enhanced progress display with validation and geographic correction info\n",
        "                pbar.set_postfix_str(\n",
        "                    f\"‚úÖ{enhanced_gpt4_stats['api_successes']}/{enhanced_gpt4_stats['total_processed']} | üìä{success_rate:.1f}% | üåç{enhanced_gpt4_stats['geographic_corrections_applied']} | üìè{enhanced_gpt4_stats['format_compliance_enforced']} | ETA:{eta_display}\"\n",
        "                )\n",
        "\n",
        "                # Progress checkpoint every 10 items with enhanced metrics\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    rate_summary = api_rate_limiter.get_rate_limit_status()\n",
        "                    pbar.set_postfix_str(f\"Checkpoint: {i+1}/{len(enriched_data)} | Geo: {enhanced_gpt4_stats['geographic_corrections_applied']} | Valid: {enhanced_gpt4_stats['validation_passes']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to process enhanced GPT-4o refinement for {item.get('filename', 'unknown')}: {e}\")\n",
        "                # Add item with fallback data\n",
        "                item.update({\n",
        "                    \"title\": \"Processing Error\",\n",
        "                    \"keywords\": item.get('specialist_keywords', ''),\n",
        "                    \"model_version\": \"error\"\n",
        "                })\n",
        "                refined_data.append(item)\n",
        "                enhanced_gpt4_stats['api_failures'] += 1\n",
        "                enhanced_gpt4_stats['total_processed'] += 1\n",
        "                pbar.set_postfix_str(f\"‚ùå Error: {item.get('filename', 'unknown')}\")\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    pipeline_timer.end_step(\"Enhanced GPT-4o Refinement\")\n",
        "\n",
        "    # Calculate enhanced final statistics\n",
        "    total_processed = enhanced_gpt4_stats['total_processed']\n",
        "    api_success_rate = (enhanced_gpt4_stats['api_successes'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    parsing_success_rate = (enhanced_gpt4_stats['parsing_successes'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    fallback_rate = (enhanced_gpt4_stats['fallback_used'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    validation_rate = (enhanced_gpt4_stats['validation_passes'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    geographic_correction_rate = (enhanced_gpt4_stats['geographic_corrections_applied'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    format_compliance_rate = (enhanced_gpt4_stats['format_compliance_enforced'] / total_processed * 100) if total_processed > 0 else 0\n",
        "    avg_keywords_per_image = (total_keywords_generated / total_processed) if total_processed > 0 else 0\n",
        "\n",
        "    avg_api_time = (enhanced_gpt4_stats['total_api_time'] / enhanced_gpt4_stats['api_successes']) if enhanced_gpt4_stats['api_successes'] > 0 else 0\n",
        "    avg_parsing_time = (enhanced_gpt4_stats['total_parsing_time'] / enhanced_gpt4_stats['parsing_successes']) if enhanced_gpt4_stats['parsing_successes'] > 0 else 0\n",
        "\n",
        "    # Get final rate limiting summary\n",
        "    final_rate_summary = api_rate_limiter.get_rate_limit_status()\n",
        "\n",
        "    if validate_data_structure(refined_data, \"Step 9 - Enhanced Output\"):\n",
        "        print(f\"\\n‚úÖ Enhanced GPT-4o Vision refinement with geographic correction complete!\")\n",
        "        print(f\"üìä ENHANCED GPT-4o PROCESSING STATISTICS:\")\n",
        "        print(f\"   üéØ Total Processed: {total_processed}\")\n",
        "        print(f\"   ü§ñ Model Used: gpt-4o-enhanced with geographic correction\")\n",
        "        print(f\"   üåê API Successes: {enhanced_gpt4_stats['api_successes']} ({api_success_rate:.1f}%)\")\n",
        "        print(f\"   ‚ùå API Failures: {enhanced_gpt4_stats['api_failures']}\")\n",
        "        print(f\"   üîç Parsing Successes: {enhanced_gpt4_stats['parsing_successes']} ({parsing_success_rate:.1f}%)\")\n",
        "        print(f\"   ‚úÖ Validation Passes: {enhanced_gpt4_stats['validation_passes']} ({validation_rate:.1f}%)\")\n",
        "        print(f\"   üîÑ Enhanced Fallback Usage: {enhanced_gpt4_stats['fallback_used']} instances ({fallback_rate:.1f}%)\")\n",
        "        print(f\"   üåç Geographic Corrections Applied: {enhanced_gpt4_stats['geographic_corrections_applied']} ({geographic_correction_rate:.1f}%)\")\n",
        "        print(f\"   üìè Format Compliance Enforced: {enhanced_gpt4_stats['format_compliance_enforced']} ({format_compliance_rate:.1f}%)\")\n",
        "        print(f\"   üè∑Ô∏è Average Keywords per Image: {avg_keywords_per_image:.1f}\")\n",
        "        print(f\"   üîÅ Model Retries: {enhanced_gpt4_stats['model_retries']}\")\n",
        "        print(f\"   ‚è±Ô∏è Avg API Time: {avg_api_time:.2f}s\")\n",
        "        print(f\"   üìù Avg Parsing Time: {avg_parsing_time:.3f}s\")\n",
        "        print(f\"   üõ°Ô∏è ENHANCED RATE LIMITING SUMMARY:\")\n",
        "        print(f\"      ‚Ä¢ Total API Requests: {final_rate_summary.get('total_requests', 0)}\")\n",
        "        print(f\"      ‚Ä¢ Throttle Events: {final_rate_summary.get('throttle_events', 0)}\")\n",
        "        print(f\"      ‚Ä¢ Estimated Cost: {final_rate_summary.get('estimated_cost', '$0.000')}\")\n",
        "        print(f\"      ‚Ä¢ Cost Savings: {final_rate_summary.get('cost_savings', '$0.000')}\")\n",
        "        print(f\"      ‚Ä¢ Avg Throttle Delay: {final_rate_summary.get('avg_throttle_delay', '0.00s')}\")\n",
        "        print(f\"      ‚Ä¢ Final Usage: {final_rate_summary.get('current_minute_usage', '0/25')} (minute) | {final_rate_summary.get('current_hour_usage', '0/1500')} (hour)\")\n",
        "        print(f\"   ‚è±Ô∏è Step Duration: {pipeline_timer.get_step_duration('Enhanced GPT-4o Refinement'):.1f} seconds\")\n",
        "        print(f\"   üöÄ Ready for final assembly with enhanced quality!\")\n",
        "\n",
        "        # Export enhanced GPT-4 results to CSV\n",
        "        print(f\"\\nüìä PHASE 6: Enhanced GPT-4 CSV Export\")\n",
        "        print(\"=\" * 45)\n",
        "\n",
        "        if refined_data:\n",
        "            enhanced_gpt4_csv_filename = \"enhanced_gpt4_keywords_output.csv\"\n",
        "\n",
        "            try:\n",
        "                print(f\"üìù Exporting {len(refined_data)} enhanced GPT-4 results to {enhanced_gpt4_csv_filename}...\")\n",
        "\n",
        "                # Create CSV with enhanced GPT-4-specific columns\n",
        "                with open(enhanced_gpt4_csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    fieldnames = [\n",
        "                        'filename',\n",
        "                        'enhanced_title',\n",
        "                        'enhanced_keywords',\n",
        "                        'original_llava_keywords',\n",
        "                        'keyword_count',\n",
        "                        'geographic_corrected',\n",
        "                        'format_compliant',\n",
        "                        'validation_passed',\n",
        "                        'processing_time',\n",
        "                        'success_status'\n",
        "                    ]\n",
        "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                    writer.writeheader()\n",
        "\n",
        "                    successful_exports = 0\n",
        "\n",
        "                    for item in refined_data:\n",
        "                        try:\n",
        "                            # Clean and prepare enhanced keywords\n",
        "                            enhanced_keywords = item.get('keywords', '')\n",
        "                            original_keywords = item.get('specialist_keywords', '')\n",
        "\n",
        "                            if 'processing_failed' in enhanced_keywords or 'Processing Failed' in item.get('title', ''):\n",
        "                                success_status = 'failed'\n",
        "                            else:\n",
        "                                success_status = 'success'\n",
        "\n",
        "                            # Extract enhanced debug information\n",
        "                            debug_info = item.get('enhanced_gpt4_debug', {})\n",
        "                            phases = debug_info.get('phases', {})\n",
        "                            processing_time = phases.get('api_call', 0) + phases.get('parsing', 0)\n",
        "\n",
        "                            writer.writerow({\n",
        "                                'filename': item.get('filename', 'unknown'),\n",
        "                                'enhanced_title': item.get('title', 'Unknown'),\n",
        "                                'enhanced_keywords': enhanced_keywords,\n",
        "                                'original_llava_keywords': original_keywords[:100] + '...' if len(str(original_keywords)) > 100 else original_keywords,  # Truncate for readability\n",
        "                                'keyword_count': item.get('keyword_count', 0),\n",
        "                                'geographic_corrected': item.get('geographic_corrected', False),\n",
        "                                'format_compliant': item.get('format_compliant', False),\n",
        "                                'validation_passed': debug_info.get('success', False),\n",
        "                                'processing_time': f\"{processing_time:.3f}s\",\n",
        "                                'success_status': success_status\n",
        "                            })\n",
        "                            successful_exports += 1\n",
        "\n",
        "                        except Exception as e:\n",
        "                            logging.error(f\"Failed to export enhanced GPT-4 result for {item.get('filename', 'unknown')}: {e}\")\n",
        "                            # Write error row\n",
        "                            writer.writerow({\n",
        "                                'filename': item.get('filename', 'unknown'),\n",
        "                                'enhanced_title': 'export_error',\n",
        "                                'enhanced_keywords': 'export_error',\n",
        "                                'original_llava_keywords': 'export_error',\n",
        "                                'keyword_count': 0,\n",
        "                                'geographic_corrected': False,\n",
        "                                'format_compliant': False,\n",
        "                                'validation_passed': False,\n",
        "                                'processing_time': '0.000s',\n",
        "                                'success_status': 'export_failed'\n",
        "                            })\n",
        "\n",
        "                # Verify export\n",
        "                if os.path.exists(enhanced_gpt4_csv_filename):\n",
        "                    file_size = os.path.getsize(enhanced_gpt4_csv_filename)\n",
        "                    print(f\"‚úÖ Enhanced GPT-4 CSV export successful!\")\n",
        "                    print(f\"   üìÅ File: {enhanced_gpt4_csv_filename}\")\n",
        "                    print(f\"   üìä Records exported: {successful_exports}/{len(refined_data)}\")\n",
        "                    print(f\"   üíæ File size: {file_size:,} bytes\")\n",
        "                    print(f\"   üîß Includes geographic correction and format compliance data\")\n",
        "\n",
        "                    # Provide download link\n",
        "                    print(f\"\\n‚¨áÔ∏è DOWNLOADING ENHANCED GPT-4 CSV RESULTS...\")\n",
        "                    bulletproof_download_csv(enhanced_gpt4_csv_filename, \"Enhanced GPT-4 Keywords Output\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed to create {enhanced_gpt4_csv_filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Enhanced GPT-4 CSV export failed: {e}\")\n",
        "                logging.error(f\"Enhanced GPT-4 CSV export error: {e}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No enhanced refined results to export\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Enhanced GPT-4o refinement completed with errors.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No valid data for enhanced GPT-4o refinement.\")\n",
        "    refined_data = enriched_data"
      ],
      "metadata": {
        "id": "mz7jF6X8gcCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10_header"
      },
      "source": [
        "## Step 10: Assemble Final Metadata Rows (WITH ERROR HANDLING)\n",
        "Transform refined data into Shopify-compatible CSV format with comprehensive validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "step10_assemble_metadata"
      },
      "outputs": [],
      "source": [
        "print(\"üìã PHASE 6: Final Metadata Assembly\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "final_csv_data = []\n",
        "assembly_stats = {\n",
        "    'processed': 0,\n",
        "    'successful': 0,\n",
        "    'errors': 0,\n",
        "    'keyword_cleaning_applied': 0,\n",
        "    'total_keywords_generated': 0\n",
        "}\n",
        "\n",
        "if validate_data_structure(refined_data, \"Step 10 - Input\"):\n",
        "    print(f\"üîÑ Assembling final metadata for {len(refined_data)} images\")\n",
        "\n",
        "    for item in refined_data:\n",
        "        try:\n",
        "            # Extract and clean keywords with comprehensive error handling\n",
        "            raw_keywords = item.get('keywords', item.get('specialist_keywords', ''))\n",
        "            if raw_keywords:\n",
        "                cleaned_keywords = clean_ai_keywords(raw_keywords)\n",
        "                assembly_stats['keyword_cleaning_applied'] += 1\n",
        "                assembly_stats['total_keywords_generated'] += len(cleaned_keywords)\n",
        "            else:\n",
        "                cleaned_keywords = ['agricultural', 'farming']\n",
        "\n",
        "            keywords_str = \", \".join(cleaned_keywords) if cleaned_keywords else \"agricultural, farming\"\n",
        "\n",
        "            # Extract core metadata with fallbacks\n",
        "            title = item.get('title', 'Agricultural Image')\n",
        "            sku = item.get('sku', '')\n",
        "            filename = item.get('filename', 'unknown.jpg')\n",
        "            vendor = item.get('vendor', 'Unknown')\n",
        "            photographer_match = item.get('photographer_match', 'no')\n",
        "\n",
        "            # Assemble final CSV row (CORRECTED FORMAT - no photographer_state)\n",
        "            final_csv_data.append({\n",
        "                'filename': filename,\n",
        "                'title': f\"{title} {sku}\".strip(),\n",
        "                'description': 'All standard image downloads are 2592 px long and available as a download link immediately after purchase. If you choose to buy a high-resolution image, it will be emailed to you within 24 hours.',\n",
        "                'keywords': keywords_str,\n",
        "                'tags': keywords_str,\n",
        "                'collections': '',\n",
        "                'models': '',\n",
        "                'vendor': vendor,  # CORRECTED: vendor contains photographer name\n",
        "                'Photographer_Match': photographer_match  # CORRECTED: yes/no only, no state column\n",
        "            })\n",
        "\n",
        "            assembly_stats['successful'] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process final metadata for {item.get('filename', 'unknown')}: {e}\")\n",
        "            # Add minimal valid row to maintain data integrity\n",
        "            final_csv_data.append({\n",
        "                'filename': item.get('filename', 'unknown.jpg'),\n",
        "                'title': 'Processing Error',\n",
        "                'description': 'Error occurred during processing',\n",
        "                'keywords': 'error, processing',\n",
        "                'tags': 'error, processing',\n",
        "                'collections': '',\n",
        "                'models': '',\n",
        "                'vendor': 'Unknown',\n",
        "                'Photographer_Name': 'Unknown',\n",
        "                'Photographer_Match': 'no'\n",
        "            })\n",
        "            assembly_stats['errors'] += 1\n",
        "\n",
        "        assembly_stats['processed'] += 1\n",
        "\n",
        "    # Final validation and statistics\n",
        "    success_rate = (assembly_stats['successful'] / assembly_stats['processed'] * 100) if assembly_stats['processed'] > 0 else 0\n",
        "    avg_keywords_per_image = (assembly_stats['total_keywords_generated'] / assembly_stats['successful']) if assembly_stats['successful'] > 0 else 0\n",
        "\n",
        "    print(f\"\\n‚úÖ Final metadata assembly complete!\")\n",
        "    print(f\"üìä ASSEMBLY STATISTICS:\")\n",
        "    print(f\"   üìù Total Records: {len(final_csv_data)}\")\n",
        "    print(f\"   ‚úÖ Successfully Processed: {assembly_stats['successful']} ({success_rate:.1f}%)\")\n",
        "    print(f\"   üßπ Keyword Cleaning Applied: {assembly_stats['keyword_cleaning_applied']}\")\n",
        "    print(f\"   üè∑Ô∏è Total Keywords Generated: {assembly_stats['total_keywords_generated']}\")\n",
        "    print(f\"   üìä Avg Keywords/Image: {avg_keywords_per_image:.1f}\")\n",
        "    print(f\"   ‚ùå Errors: {assembly_stats['errors']}\")\n",
        "    print(f\"   üöÄ Ready for CSV export!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No valid data for final assembly.\")\n",
        "    final_csv_data = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Final CSV Export (Shopify-Compatible)\n",
        "Export the final processed data to a Shopify-compatible CSV file with comprehensive validation and bulletproof download."
      ],
      "metadata": {
        "id": "Mhaf_g45gcCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ PHASE 7: Final CSV Export (Shopify-Compatible)\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "pipeline_timer.start_step(\"Final CSV Export\")\n",
        "\n",
        "def export_to_shopify_csv_v4_simplified(data, filename=\"final_shopify_export.csv\"):\n",
        "    \"\"\"Export data to Shopify-compatible CSV with comprehensive validation\"\"\"\n",
        "    export_stats = {\n",
        "        'total_records': len(data),\n",
        "        'successful_exports': 0,\n",
        "        'failed_exports': 0,\n",
        "        'validation_errors': 0,\n",
        "        'character_limit_violations': 0\n",
        "    }\n",
        "\n",
        "    if not data:\n",
        "        logging.error(\"No data provided for export\")\n",
        "        return False, export_stats\n",
        "\n",
        "    print(f\"üìä Exporting {len(data)} records to Shopify-compatible CSV...\")\n",
        "\n",
        "    # Shopify-compatible field names (9 columns as per BUILD_SPEC)\n",
        "    fieldnames = [\n",
        "        'filename',\n",
        "        'title',\n",
        "        'description',\n",
        "        'keywords',\n",
        "        'tags',\n",
        "        'collections',\n",
        "        'models',\n",
        "        'vendor',\n",
        "        'photographer_match'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "            for item in data:\n",
        "                try:\n",
        "                    # Extract SKU from filename for title appending\n",
        "                    filename = item.get('filename', 'unknown.jpg')\n",
        "                    sku = None\n",
        "\n",
        "                    # Extract numeric SKU from filename (e.g., 'HS 115642.jpg' -> '115642')\n",
        "                    import re\n",
        "                    sku_patterns = [\n",
        "                        r'[A-Z]{2} (\\d{6})',     # Primary: AH 270480\n",
        "                        r'([A-Z]{2})(\\d{6})',    # Secondary: AH270480\n",
        "                        r'(\\d{6})',              # Basic: 270480\n",
        "                        r'[A-Z]+_(\\d{5,7})',     # Variable length: AB_12345\n",
        "                        r'([A-Z]+)(\\d{5,7})',    # No underscore: AB12345\n",
        "                        r'\b(\\d{5,7})\b',        # Word boundary: any 5-7 digits\n",
        "                    ]\n",
        "\n",
        "                    for pattern in sku_patterns:\n",
        "                        match = re.search(pattern, filename)\n",
        "                        if match:\n",
        "                            if len(match.groups()) == 1:\n",
        "                                sku = match.group(1)\n",
        "                            elif len(match.groups()) == 2:\n",
        "                                sku = match.group(2)  # Return the numeric part\n",
        "                            break\n",
        "\n",
        "                    # Create title with SKU appended\n",
        "                    base_title = str(item.get('title', 'Agricultural Image'))\n",
        "                    if sku:\n",
        "                        title = f'{base_title} {sku}'\n",
        "                    else:\n",
        "                        title = base_title\n",
        "\n",
        "                    title = title[:70]  # Enforce 70 char limit\n",
        "                    if len(str(item.get('title', ''))) > 70:\n",
        "                        export_stats['character_limit_violations'] += 1\n",
        "\n",
        "                    keywords = str(item.get('keywords', ''))[:500]  # Enforce 500 char limit\n",
        "                    if len(str(item.get('keywords', ''))) > 500:\n",
        "                        export_stats['character_limit_violations'] += 1\n",
        "\n",
        "                    description = str(item.get('description', 'All standard image downloads are 2592 px long and available as a download link immediately after purchase. If you choose to buy a high-resolution image, it will be emailed to you within 24 hours.'))\n",
        "\n",
        "                    # Use bulletproof string escaping for safety\n",
        "                    safe_title = bulletproof_escape_string(title)\n",
        "                    safe_keywords = bulletproof_escape_string(keywords)\n",
        "                    safe_description = bulletproof_escape_string(description)\n",
        "\n",
        "                    # Write the row with exactly 9 columns\n",
        "                    writer.writerow({\n",
        "                        'filename': item.get('filename', 'unknown.jpg'),\n",
        "                        'title': safe_title,\n",
        "                        'description': safe_description,\n",
        "                        'keywords': safe_keywords,\n",
        "                        'tags': safe_keywords,  # Use same as keywords\n",
        "                        'collections': item.get('collections', ''),\n",
        "                        'models': item.get('models', ''),\n",
        "                        'vendor': item.get('vendor', 'Unknown'),\n",
        "                        'photographer_match': item.get('photographer_match', 'no')\n",
        "                    })\n",
        "\n",
        "                    export_stats['successful_exports'] += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to export record {item.get('filename', 'unknown')}: {e}\")\n",
        "                    export_stats['failed_exports'] += 1\n",
        "                    export_stats['validation_errors'] += 1\n",
        "\n",
        "        # Validate the exported file\n",
        "        if os.path.exists(filename):\n",
        "            file_size = os.path.getsize(filename)\n",
        "\n",
        "            # Verify row count\n",
        "            with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "                reader = csv.reader(csvfile)\n",
        "                row_count = sum(1 for row in reader) - 1  # Subtract header\n",
        "\n",
        "            pipeline_timer.end_step(\"Final CSV Export\")\n",
        "\n",
        "            print(f\"‚úÖ Shopify CSV export successful!\")\n",
        "            print(f\"üìä EXPORT STATISTICS:\")\n",
        "            print(f\"   üìÅ File: {filename}\")\n",
        "            print(f\"   üìù Total Records: {export_stats['total_records']}\")\n",
        "            print(f\"   ‚úÖ Successful Exports: {export_stats['successful_exports']}\")\n",
        "            print(f\"   ‚ùå Failed Exports: {export_stats['failed_exports']}\")\n",
        "            print(f\"   ‚ö†Ô∏è Character Limit Violations: {export_stats['character_limit_violations']}\")\n",
        "            print(f\"   üíæ File Size: {file_size:,} bytes\")\n",
        "            print(f\"   üìä CSV Rows: {row_count}\")\n",
        "            print(f\"   ‚è±Ô∏è Export Duration: {pipeline_timer.get_step_duration('Final CSV Export'):.1f} seconds\")\n",
        "\n",
        "            return True, export_stats\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to create export file: {filename}\")\n",
        "            return False, export_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"CSV export failed: {e}\")\n",
        "        print(f\"‚ùå Export operation failed: {e}\")\n",
        "        return False, export_stats\n",
        "\n",
        "\n",
        "def validate_csv_row(row_data):\n",
        "    \"\"\"Validate a single CSV row for Shopify compatibility\"\"\"\n",
        "\n",
        "    validation_errors = []\n",
        "\n",
        "    # Filename validation\n",
        "    if not row_data.get('filename'):\n",
        "        validation_errors.append(\"Missing filename\")\n",
        "    elif not row_data['filename'].lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        validation_errors.append(\"Invalid filename extension\")\n",
        "\n",
        "    # Title validation\n",
        "    title = row_data.get('title', '')\n",
        "    if len(title) > 70:\n",
        "        validation_errors.append(f\"Title too long ({len(title)} chars, max 70)\")\n",
        "        row_data['title'] = title[:67] + \"...\"  # Auto-truncate\n",
        "    elif len(title) < 5:\n",
        "        validation_errors.append(\"Title too short (minimum 5 characters)\")\n",
        "\n",
        "    # Keywords validation\n",
        "    keywords = row_data.get('keywords', '')\n",
        "    if len(keywords) > 500:\n",
        "        validation_errors.append(f\"Keywords too long ({len(keywords)} chars, max 500)\")\n",
        "        # Auto-truncate at last complete keyword\n",
        "        truncated = keywords[:500]\n",
        "        last_comma = truncated.rfind(',')\n",
        "        if last_comma > 0:\n",
        "            row_data['keywords'] = truncated[:last_comma]\n",
        "\n",
        "    keyword_count = len(keywords.split(',')) if keywords else 0\n",
        "    if keyword_count < 8:\n",
        "        validation_errors.append(f\"Too few keywords ({keyword_count}, minimum 8)\")\n",
        "    elif keyword_count > 12:\n",
        "        validation_errors.append(f\"Too many keywords ({keyword_count}, maximum 12)\")\n",
        "\n",
        "    # Vendor validation\n",
        "    vendor = row_data.get('vendor', '')\n",
        "    if not vendor or vendor.strip() == '':\n",
        "        row_data['vendor'] = 'Unknown'\n",
        "\n",
        "    # Photographer match validation\n",
        "    photographer_match = row_data.get('photographer_match', '').lower()\n",
        "    if photographer_match not in ['yes', 'no']:\n",
        "        row_data['photographer_match'] = 'no'\n",
        "\n",
        "    return validation_errors, row_data\n",
        "\n",
        "# Execute final CSV export\n",
        "print(\"üîÑ Preparing final data for Shopify export...\")\n",
        "\n",
        "if validate_data_structure(final_csv_data, \"Step 11 - Final Export\"):\n",
        "    export_success, export_stats = export_to_shopify_csv_v4_simplified(\n",
        "        final_csv_data,\n",
        "        \"final_shopify_export.csv\"\n",
        "    )\n",
        "\n",
        "    if export_success:\n",
        "        print(f\"\\n‚¨áÔ∏è DOWNLOADING FINAL SHOPIFY CSV...\")\n",
        "        download_success = bulletproof_download_csv(\n",
        "            \"final_shopify_export.csv\",\n",
        "            \"Final Shopify Export\"\n",
        "        )\n",
        "\n",
        "        if download_success:\n",
        "            print(\"‚úÖ Final CSV successfully downloaded!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Download failed - file available locally\")\n",
        "    else:\n",
        "        print(\"‚ùå Final CSV export failed\")\n",
        "        print(\"üí° Check data integrity and try manual export\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No valid data available for final export\")\n",
        "    print(\"üîÑ Pipeline completed with limited data - check previous steps\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 55)\n",
        "print(\"üèÅ Step 11 Complete: Final CSV Export Done\")"
      ],
      "metadata": {
        "id": "step11_csv_export"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Pipeline Summary and Quality Assurance\n",
        "Comprehensive performance metrics, quality assurance reporting, and complete pipeline statistics."
      ],
      "metadata": {
        "id": "CjHuh8PkgcCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üéØ FINAL PHASE: Pipeline Summary & Quality Assurance\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pipeline_timer.start_step(\"Pipeline Summary\")\n",
        "\n",
        "def generate_comprehensive_pipeline_summary():\n",
        "    \"\"\"Generate comprehensive pipeline performance and quality metrics\"\"\"\n",
        "\n",
        "    summary = {\n",
        "        'pipeline_info': {\n",
        "            'version': 'AGStock Keyworder Enhanced v4.0 FIXED',\n",
        "            'completion_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'total_pipeline_time': pipeline_timer.get_total_time()\n",
        "        },\n",
        "        'step_performance': {},\n",
        "        'data_quality': {},\n",
        "        'resource_usage': {},\n",
        "        'success_metrics': {}\n",
        "    }\n",
        "\n",
        "    # Step-by-step performance analysis\n",
        "    print(\"üìä STEP-BY-STEP PERFORMANCE ANALYSIS:\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    step_names = [\n",
        "        \"Dropbox Integration\",\n",
        "        \"Model Loading\",\n",
        "        \"LLaVA Specialist Inference\",\n",
        "        \"Final CSV Export\",\n",
        "        \"Pipeline Summary\"\n",
        "    ]\n",
        "\n",
        "    total_step_time = 0\n",
        "    for step in step_names:\n",
        "        duration = pipeline_timer.get_step_duration(step)\n",
        "        summary['step_performance'][step] = duration\n",
        "        total_step_time += duration\n",
        "\n",
        "        if duration > 0:\n",
        "            print(f\"   ‚è±Ô∏è {step:<25}: {duration:>8.1f}s\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è {step:<25}: {'Not tracked':>8s}\")\n",
        "\n",
        "    # Data quality assessment\n",
        "    print(f\"\\nüìã DATA QUALITY ASSESSMENT:\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    # Count successful processing at each stage\n",
        "    specialist_count = len(specialist_results) if 'specialist_results' in globals() else 0\n",
        "    enriched_count = len(enriched_data) if 'enriched_data' in globals() else 0\n",
        "    refined_count = len(refined_data) if 'refined_data' in globals() else 0\n",
        "    final_count = len(final_csv_data) if 'final_csv_data' in globals() else 0\n",
        "\n",
        "    summary['data_quality'] = {\n",
        "        'specialist_inference': specialist_count,\n",
        "        'data_enrichment': enriched_count,\n",
        "        'gpt4_refinement': refined_count,\n",
        "        'final_assembly': final_count,\n",
        "        'data_retention_rate': (final_count / specialist_count * 100) if specialist_count > 0 else 0\n",
        "    }\n",
        "\n",
        "    print(f\"   ü§ñ Specialist Inference Results: {specialist_count:>6}\")\n",
        "    print(f\"   üîÑ Data Enrichment Results:     {enriched_count:>6}\")\n",
        "    print(f\"   üéØ GPT-4 Refinement Results:    {refined_count:>6}\")\n",
        "    print(f\"   üìã Final Assembly Results:      {final_count:>6}\")\n",
        "    print(f\"   üìä Data Retention Rate:         {(final_count / specialist_count * 100) if specialist_count > 0 else 0:>5.1f}%\")\n",
        "\n",
        "    # Resource usage analysis\n",
        "    print(f\"\\nüíæ RESOURCE USAGE ANALYSIS:\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    current_memory = get_gpu_memory_usage()\n",
        "    system_memory = psutil.virtual_memory()\n",
        "\n",
        "    summary['resource_usage'] = {\n",
        "        'final_gpu_memory': current_memory,\n",
        "        'system_memory_used': system_memory.percent,\n",
        "        'system_memory_available': system_memory.available / (1024**3)  # GB\n",
        "    }\n",
        "\n",
        "    print(f\"   üñ•Ô∏è Final GPU Memory Usage:      {current_memory:>5.2f}GB\")\n",
        "    print(f\"   üíª System Memory Usage:         {system_memory.percent:>5.1f}%\")\n",
        "    print(f\"   üíæ Available System Memory:     {system_memory.available / (1024**3):>5.1f}GB\")\n",
        "\n",
        "    # Success metrics and quality scores\n",
        "    print(f\"\\n‚úÖ SUCCESS METRICS & QUALITY SCORES:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Calculate success rates\n",
        "    llava_success_rate = 0\n",
        "    gpt4_success_rate = 0\n",
        "    export_success_rate = 0\n",
        "\n",
        "    if 'processing_stats' in globals():\n",
        "        total_processed = processing_stats.get('successful_inferences', 0) + processing_stats.get('failed_inferences', 0)\n",
        "        llava_success_rate = (processing_stats.get('successful_inferences', 0) / total_processed * 100) if total_processed > 0 else 0\n",
        "\n",
        "    if 'gpt4_stats' in globals():\n",
        "        gpt4_total = gpt4_stats.get('total_processed', 0)\n",
        "        gpt4_success_rate = (gpt4_stats.get('api_successes', 0) / gpt4_total * 100) if gpt4_total > 0 else 0\n",
        "\n",
        "    if 'export_stats' in globals():\n",
        "        export_total = export_stats.get('total_records', 0)\n",
        "        export_success_rate = (export_stats.get('successful_exports', 0) / export_total * 100) if export_total > 0 else 0\n",
        "\n",
        "    summary['success_metrics'] = {\n",
        "        'llava_success_rate': llava_success_rate,\n",
        "        'gpt4_success_rate': gpt4_success_rate,\n",
        "        'export_success_rate': export_success_rate,\n",
        "        'overall_pipeline_success': (llava_success_rate + gpt4_success_rate + export_success_rate) / 3\n",
        "    }\n",
        "\n",
        "    print(f\"   ü§ñ LLaVA Success Rate:           {llava_success_rate:>5.1f}%\")\n",
        "    print(f\"   üéØ GPT-4 Success Rate:           {gpt4_success_rate:>5.1f}%\")\n",
        "    print(f\"   üìä Export Success Rate:          {export_success_rate:>5.1f}%\")\n",
        "    print(f\"   üèÜ Overall Pipeline Success:     {(llava_success_rate + gpt4_success_rate + export_success_rate) / 3:>5.1f}%\")\n",
        "\n",
        "    # Quality assurance recommendations\n",
        "    print(f\"\\nüîç QUALITY ASSURANCE RECOMMENDATIONS:\")\n",
        "    print(\"=\" * 42)\n",
        "\n",
        "    recommendations = []\n",
        "\n",
        "    if llava_success_rate < 90:\n",
        "        recommendations.append(\"‚ö†Ô∏è LLaVA inference success rate below 90% - check model performance\")\n",
        "    if gpt4_success_rate < 80:\n",
        "        recommendations.append(\"‚ö†Ô∏è GPT-4 API success rate below 80% - verify API keys and quotas\")\n",
        "    if export_success_rate < 95:\n",
        "        recommendations.append(\"‚ö†Ô∏è Export success rate below 95% - check data validation\")\n",
        "    if final_count < specialist_count * 0.9:\n",
        "        recommendations.append(\"‚ö†Ô∏è High data loss through pipeline - review error handling\")\n",
        "\n",
        "    if not recommendations:\n",
        "        print(\"   ‚úÖ All quality metrics within acceptable ranges!\")\n",
        "        print(\"   üéâ Pipeline performed excellently!\")\n",
        "    else:\n",
        "        for rec in recommendations:\n",
        "            print(f\"   {rec}\")\n",
        "\n",
        "    summary['quality_recommendations'] = recommendations\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Generate and display comprehensive summary\n",
        "pipeline_summary = generate_comprehensive_pipeline_summary()\n",
        "\n",
        "pipeline_timer.end_step(\"Pipeline Summary\")\n",
        "\n",
        "print(f\"\\nüéâ PIPELINE COMPLETION SUMMARY:\")\n",
        "print(\"=\" * 35)\n",
        "print(f\"   üöÄ Version: {pipeline_summary['pipeline_info']['version']}\")\n",
        "print(f\"   ‚è∞ Completed: {pipeline_summary['pipeline_info']['completion_time']}\")\n",
        "print(f\"   ‚è±Ô∏è Total Time: {pipeline_summary['pipeline_info']['total_pipeline_time']:.1f} seconds\")\n",
        "print(f\"   üìä Final Records: {pipeline_summary['data_quality']['final_assembly']}\")\n",
        "print(f\"   üèÜ Overall Success: {pipeline_summary['success_metrics']['overall_pipeline_success']:.1f}%\")\n",
        "\n",
        "# Save summary to JSON for record keeping\n",
        "summary_filename = f\"pipeline_summary_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "try:\n",
        "    with open(summary_filename, 'w') as f:\n",
        "        json.dump(pipeline_summary, f, indent=2, default=str)\n",
        "    print(f\"   üìÅ Summary saved: {summary_filename}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Failed to save summary: {e}\")\n",
        "\n",
        "print(f\"\\nüèÅ MASTER AGRICULTURAL KEYWORDING PIPELINE COMPLETE! üèÅ\")\n",
        "print(\"=\" * 65)\n",
        "print(\"üåæ Thank you for using AGStock Keyworder Enhanced v4.0 FIXED! üåæ\")\n",
        "print(\"üöú Professional agricultural image keywording at your service! üöú\")\n",
        "print(\"=\" * 65)"
      ],
      "metadata": {
        "id": "step12_pipeline_summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Integration Testing & Validation Verification\n",
        "Comprehensive integration tests to verify all validation functions work correctly in the full pipeline context."
      ],
      "metadata": {
        "id": "UX39Qp5pgcCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß™ INTEGRATION TESTING & VALIDATION VERIFICATION\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "import unittest\n",
        "from unittest.mock import MagicMock, patch\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "class IntegrationTestSuite(unittest.TestCase):\n",
        "    \"\"\"Comprehensive integration tests for the agricultural keywording pipeline\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up test environment once for all tests\"\"\"\n",
        "        cls.test_data_dir = tempfile.mkdtemp()\n",
        "        cls.test_image_path = os.path.join(cls.test_data_dir, \"test_image.jpg\")\n",
        "\n",
        "        # Create a minimal test image\n",
        "        from PIL import Image\n",
        "        test_img = Image.new('RGB', (100, 100), color='green')\n",
        "        test_img.save(cls.test_image_path)\n",
        "\n",
        "        print(f\"üèóÔ∏è Test environment created: {cls.test_data_dir}\")\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up test environment\"\"\"\n",
        "        shutil.rmtree(cls.test_data_dir)\n",
        "        print(\"üßπ Test environment cleaned up\")\n",
        "\n",
        "    def test_keyword_filtering_integration(self):\n",
        "        \"\"\"Test LLaVA keyword filtering with real data\"\"\"\n",
        "        print(\"üîç Testing keyword filtering integration...\")\n",
        "\n",
        "        # Test with typical LLaVA overflow output\n",
        "        raw_keywords = \"angus, maryland, usa, cattle, cattle cattle, cow, angus cattle, animal agriculture, black angus cattle, beef cattle, livestock, farm, farming, agriculture, rural, pastoral, processing, computer, technical, operation focus\"\n",
        "\n",
        "        filtered = filter_llava_keywords(raw_keywords, max_keywords=12)\n",
        "        filtered_list = [kw.strip() for kw in filtered.split(',')]\n",
        "\n",
        "        # Assertions\n",
        "        self.assertLessEqual(len(filtered_list), 12, \"Should limit to max 12 keywords\")\n",
        "        self.assertGreaterEqual(len(filtered_list), 8, \"Should have at least 8 keywords\")\n",
        "        self.assertIn('maryland', filtered_list, \"Should preserve geographic terms\")\n",
        "        self.assertNotIn('processing', filtered_list, \"Should remove technical terms\")\n",
        "\n",
        "        print(f\"   ‚úÖ Filtered {len(raw_keywords.split(','))} ‚Üí {len(filtered_list)} keywords\")\n",
        "\n",
        "    def test_geographic_correction_integration(self):\n",
        "        \"\"\"Test geographic correction with photographer context\"\"\"\n",
        "        print(\"üåç Testing geographic correction integration...\")\n",
        "\n",
        "        keywords = \"iowa, corn, agriculture, farming\"  # Wrong state\n",
        "        photographer_context = {'state': 'wisconsin', 'photographer': 'TestPhotographer'}\n",
        "\n",
        "        corrected = correct_geographic_keywords(keywords, photographer_context)\n",
        "        corrected_list = [kw.strip() for kw in corrected.split(',')]\n",
        "\n",
        "        # Assertions\n",
        "        self.assertEqual(corrected_list[0], 'wisconsin', \"Should place correct state first\")\n",
        "        self.assertNotIn('iowa', corrected_list, \"Should remove incorrect states\")\n",
        "        self.assertIn('corn', corrected_list, \"Should preserve non-geographic keywords\")\n",
        "\n",
        "        print(f\"   ‚úÖ Geographic correction: iowa ‚Üí wisconsin\")\n",
        "\n",
        "    def test_redundancy_removal_integration(self):\n",
        "        \"\"\"Test redundancy removal with complex patterns\"\"\"\n",
        "        print(\"üîÑ Testing redundancy removal integration...\")\n",
        "\n",
        "        keywords = ['cattle', 'cattle cattle', 'cattle breeding', 'angus', 'processing', 'plant', 'beef']\n",
        "        filtered = remove_redundant_terms(keywords)\n",
        "\n",
        "        # Assertions\n",
        "        self.assertIn('cattle', filtered, \"Should keep base terms\")\n",
        "        self.assertIn('angus', filtered, \"Should keep specific breeds\")\n",
        "        self.assertIn('beef', filtered, \"Should keep agricultural terms\")\n",
        "        self.assertNotIn('cattle cattle', filtered, \"Should remove duplications\")\n",
        "        self.assertNotIn('processing', filtered, \"Should remove technical terms\")\n",
        "\n",
        "        print(f\"   ‚úÖ Removed {len(keywords) - len(filtered)} redundant terms\")\n",
        "\n",
        "    def test_photographer_matching_integration(self):\n",
        "        \"\"\"Test photographer matching with various SKU formats\"\"\"\n",
        "        print(\"üë• Testing photographer matching integration...\")\n",
        "\n",
        "        # Create temporary photographer CSV\n",
        "        temp_csv = os.path.join(self.test_data_dir, \"test_photographers.csv\")\n",
        "        with open(temp_csv, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['sku_range', 'name', 'state'])\n",
        "            writer.writerow(['270000-279999', 'Test Photographer', 'Wisconsin'])\n",
        "            writer.writerow(['280', 'Another Photographer', 'Iowa'])\n",
        "\n",
        "        matcher = PhotographerMatcher(temp_csv)\n",
        "\n",
        "        # Test range matching\n",
        "        result1 = matcher.get_vendor_info('270480')\n",
        "        self.assertEqual(result1['vendor'], 'Test Photographer')\n",
        "        self.assertEqual(result1['state'], 'Wisconsin')\n",
        "        self.assertEqual(result1['photographer_match'], 'yes')\n",
        "\n",
        "        # Test prefix matching\n",
        "        result2 = matcher.get_vendor_info('280123')\n",
        "        self.assertEqual(result2['vendor'], 'Another Photographer')\n",
        "\n",
        "        print(f\"   ‚úÖ Photographer matching: 2 successful matches\")\n",
        "\n",
        "    def test_csv_export_integration(self):\n",
        "        \"\"\"Test CSV export with complete data structure\"\"\"\n",
        "        print(\"üìä Testing CSV export integration...\")\n",
        "\n",
        "        test_data = [{\n",
        "            'filename': 'test_image.jpg',\n",
        "            'title': 'Wisconsin Agriculture',\n",
        "            'description': 'Test description',\n",
        "            'keywords': 'wisconsin, corn, agriculture, farming',\n",
        "            'tags': 'wisconsin, corn, agriculture, farming',\n",
        "            'collections': '',\n",
        "            'models': '',\n",
        "            'vendor': 'Test Photographer',\n",
        "            'photographer_match': 'yes'\n",
        "        }]\n",
        "\n",
        "        temp_csv = os.path.join(self.test_data_dir, \"test_export.csv\")\n",
        "        success, stats = export_to_shopify_csv_v4_simplified(test_data, temp_csv)\n",
        "\n",
        "        # Assertions\n",
        "        self.assertTrue(success, \"Export should succeed\")\n",
        "        self.assertTrue(os.path.exists(temp_csv), \"CSV file should be created\")\n",
        "        self.assertEqual(stats['successful_exports'], 1, \"Should export 1 record\")\n",
        "\n",
        "        # Verify CSV content\n",
        "        with open(temp_csv, 'r') as f:\n",
        "            content = f.read()\n",
        "            self.assertIn('wisconsin', content, \"Should contain keywords\")\n",
        "            self.assertIn('Test Photographer', content, \"Should contain vendor\")\n",
        "\n",
        "        print(f\"   ‚úÖ CSV export: 1 record exported successfully\")\n",
        "\n",
        "    def test_json_validation_integration(self):\n",
        "        \"\"\"Test JSON validation with complex nested data\"\"\"\n",
        "        print(\"üîç Testing JSON validation integration...\")\n",
        "\n",
        "        test_data = [\n",
        "            {\n",
        "                'filename': 'test.jpg',\n",
        "                'specialist_keywords': 'corn, agriculture',\n",
        "                'debug_timing': {'total': 2.5, 'inference': 1.8},\n",
        "                'debug_memory': {'pre_inference': 2.1, 'post_inference': 2.3}\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        is_valid, message = progressive_json_validation(test_data, \"Integration Test\")\n",
        "\n",
        "        # Assertions\n",
        "        self.assertTrue(is_valid, f\"JSON validation should pass: {message}\")\n",
        "\n",
        "        print(f\"   ‚úÖ JSON validation: {message}\")\n",
        "\n",
        "    def test_bulletproof_string_escaping_integration(self):\n",
        "        \"\"\"Test bulletproof string escaping with problematic content\"\"\"\n",
        "        print(\"üõ°Ô∏è Testing bulletproof string escaping integration...\")\n",
        "\n",
        "        problematic_text = 'Test \"quotes\" and\\nnewlines and\\ttabs'\n",
        "        escaped = bulletproof_escape_string(problematic_text)\n",
        "\n",
        "        # Assertions\n",
        "        self.assertNotIn('\"', escaped.replace('\\\\\"', ''))  # No unescaped quotes\n",
        "        self.assertNotIn('\\n', escaped.replace('\\\\n', ''))  # No unescaped newlines\n",
        "        self.assertIn('\\\\\"', escaped, \"Should escape quotes\")\n",
        "        self.assertIn('\\\\n', escaped, \"Should escape newlines\")\n",
        "\n",
        "        print(f\"   ‚úÖ String escaping: All special characters properly escaped\")\n",
        "\n",
        "    def test_api_rate_limiting_integration(self):\n",
        "        \"\"\"Test API rate limiting functionality\"\"\"\n",
        "        print(\"üîÑ Testing API rate limiting integration...\")\n",
        "\n",
        "        limiter = APIRateLimiter(requests_per_minute=2, requests_per_hour=10)\n",
        "\n",
        "        # Make multiple requests quickly\n",
        "        start_time = time.time()\n",
        "        for i in range(3):\n",
        "            limiter.wait_if_needed()\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Should have introduced delays\n",
        "        total_time = end_time - start_time\n",
        "        self.assertGreater(total_time, 2.0, \"Should introduce throttling delays\")\n",
        "\n",
        "        status = limiter.get_rate_limit_status()\n",
        "        self.assertEqual(status['total_requests'], 3, \"Should track requests\")\n",
        "\n",
        "        print(f\"   ‚úÖ Rate limiting: {total_time:.1f}s for 3 requests (throttling active)\")\n",
        "\n",
        "    def test_memory_management_integration(self):\n",
        "        \"\"\"Test memory management functions\"\"\"\n",
        "        print(\"üíæ Testing memory management integration...\")\n",
        "\n",
        "        initial_memory = get_gpu_memory_usage()\n",
        "        cleanup_memory()\n",
        "        post_cleanup_memory = get_gpu_memory_usage()\n",
        "\n",
        "        # Memory should be same or lower after cleanup\n",
        "        self.assertLessEqual(post_cleanup_memory, initial_memory + 0.1,\n",
        "                           \"Memory should not increase after cleanup\")\n",
        "\n",
        "        print(f\"   ‚úÖ Memory management: {initial_memory:.2f}GB ‚Üí {post_cleanup_memory:.2f}GB\")\n",
        "\n",
        "def run_integration_tests():\n",
        "    \"\"\"Run all integration tests and provide detailed reporting\"\"\"\n",
        "    print(\"üöÄ Starting Integration Test Suite...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create test suite\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(IntegrationTestSuite)\n",
        "\n",
        "    # Custom test result class for detailed reporting\n",
        "    class DetailedTestResult(unittest.TextTestResult):\n",
        "        def __init__(self, stream, descriptions, verbosity):\n",
        "            super().__init__(stream, descriptions, verbosity)\n",
        "            self.test_results = []\n",
        "\n",
        "        def addSuccess(self, test):\n",
        "            super().addSuccess(test)\n",
        "            self.test_results.append(('PASS', test._testMethodName, None))\n",
        "\n",
        "        def addFailure(self, test, err):\n",
        "            super().addFailure(test, err)\n",
        "            self.test_results.append(('FAIL', test._testMethodName, err[1]))\n",
        "\n",
        "        def addError(self, test, err):\n",
        "            super().addError(test, err)\n",
        "            self.test_results.append(('ERROR', test._testMethodName, err[1]))\n",
        "\n",
        "    # Run tests with detailed output\n",
        "    runner = unittest.TextTestRunner(verbosity=0, resultclass=DetailedTestResult)\n",
        "    result = runner.run(suite)\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    print(f\"\\nüìä INTEGRATION TEST RESULTS:\")\n",
        "    print(\"=\" * 35)\n",
        "    print(f\"   üéØ Tests Run: {result.testsRun}\")\n",
        "    print(f\"   ‚úÖ Passed: {result.testsRun - len(result.failures) - len(result.errors)}\")\n",
        "    print(f\"   ‚ùå Failed: {len(result.failures)}\")\n",
        "    print(f\"   üö® Errors: {len(result.errors)}\")\n",
        "\n",
        "    # Detailed results\n",
        "    if hasattr(result, 'test_results'):\n",
        "        print(f\"\\nüìã DETAILED TEST RESULTS:\")\n",
        "        print(\"-\" * 30)\n",
        "        for status, test_name, error in result.test_results:\n",
        "            status_icon = \"‚úÖ\" if status == \"PASS\" else \"‚ùå\"\n",
        "            print(f\"   {status_icon} {test_name}\")\n",
        "            if error and len(str(error)) < 100:\n",
        "                print(f\"      ‚îî‚îÄ {str(error)}\")\n",
        "\n",
        "    # Calculate success rate\n",
        "    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0\n",
        "\n",
        "    print(f\"\\nüèÜ INTEGRATION TEST SUMMARY:\")\n",
        "    print(f\"   üìä Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "    if success_rate >= 90:\n",
        "        print(\"   üéâ EXCELLENT: All major integration points validated!\")\n",
        "    elif success_rate >= 75:\n",
        "        print(\"   ‚úÖ GOOD: Core functionality validated with minor issues\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è NEEDS ATTENTION: Multiple integration issues detected\")\n",
        "\n",
        "    return success_rate >= 75\n",
        "\n",
        "# Execute Integration Tests\n",
        "integration_success = run_integration_tests()\n",
        "\n",
        "print(f\"\\nüîó PIPELINE INTEGRATION STATUS:\")\n",
        "print(\"=\" * 35)\n",
        "if integration_success:\n",
        "    print(\"‚úÖ All critical pipeline integrations verified\")\n",
        "    print(\"üöÄ System ready for production deployment\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Integration issues detected - review required\")\n",
        "    print(\"üîß Recommend addressing failures before production\")\n",
        "\n",
        "print(f\"\\nüèÅ Step 13 Complete: Integration Testing Finished\")\n",
        "print(\"=\" * 55)"
      ],
      "metadata": {
        "id": "OsFGrWOhgcCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}